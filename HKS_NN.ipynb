{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "45bd7c94-affc-482d-b078-4e5b800b4304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import igl \n",
    "import networkx as nx \n",
    "import numpy as np # np.linalg.eig\n",
    "import scipy as sp\n",
    "from meshplot import plot, subplot, interact\n",
    "import meshplot as mp\n",
    "import time\n",
    "\n",
    "# igl \n",
    "import os\n",
    "root_folder = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca1c1f3-defd-40a8-8cd1-82b13fdaf2a5",
   "metadata": {},
   "source": [
    "**Hyper Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "3ed2e042-8ec0-44af-b64b-f2ad5f4cb6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_dir = '\\SHREC11'\n",
    "NUM_MESHES = 600 \n",
    "NUM_GRPS = 30\n",
    "path_to_labels = os.path.join(root_folder, \"SHREC11\", \"labels.txt\")\n",
    "train_size = int(.8*NUM_MESHES) # set aside train (80% )and test (20%) data \n",
    "\n",
    "# set values of t \n",
    "# ts= [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1] # TODO: add values larger than 1 \n",
    "\n",
    "# tODO: have more values of T \n",
    "ts=np.exp(np.linspace(-3,3,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "9fbf84b7-f3eb-4efb-b893-c1589fd3777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert mesh to networkx graph\n",
    "\n",
    "def do_task(num):\n",
    "    v, f = igl.read_triangle_mesh(os.path.join(root_folder, \"SHREC11\", \"T\"+str(num)+\".obj\"))\n",
    "    # Mesh in (v,f)\n",
    "    adj_mat = igl.adjacency_matrix(f) # this gets me the adj_mat for this mesh \n",
    "    # print(\"type\", type(adj_mat)) # type <class 'scipy.sparse._csc.csc_matrix'>\n",
    "    \n",
    "    G = nx.from_scipy_sparse_array(adj_mat) # creates a new graph from an adj matrix given as a Scipy sparse array \n",
    "    # print(\"G\", G)\n",
    "    \n",
    "    L = nx.laplacian_matrix(G).toarray() # get Laplacian matrix  \n",
    "    # print(\"L\", L)\n",
    "    \n",
    "    # get eigenvalues from graph Laplacian \n",
    "    eigen_values = np.linalg.eigvals(L)\n",
    "    # print(\"eigen_values\", eigen_values)\n",
    "    # print(\"eigen_values type\", type(eigen_values)) # ndarray\n",
    "    # print(\"eigen_values len\", len(eigen_values)) # 252 nodes hence len is 252 \n",
    "    \n",
    "    # compute e^t*eigen_value\n",
    "    \n",
    "    HKS = [] \n",
    "    \n",
    "    for t in ts:\n",
    "        t_eigen_values = t*eigen_values # an array where each lambda is multiplied by t \n",
    "        # print(\"t_eigen_values\", t*eigen_values)\n",
    "\n",
    "        h_t = np.mean(np.exp(-1*t_eigen_values)) # get average to compute h(t) \n",
    "        # print(\"h_t\", h_t)\n",
    "        \n",
    "        HKS.append(h_t) \n",
    "        \n",
    "    return HKS \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "49b9bad4-4179-4618-a912-88de9327c69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 22.11951994895935 seconds\n",
      "HKS len 600\n"
     ]
    }
   ],
   "source": [
    "HKS_all_do_task = [] \n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(600):\n",
    "    \n",
    "    HKS = do_task(i)\n",
    "    HKS_all_do_task.append(HKS)\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "time_taken = end_time-start_time\n",
    "\n",
    "print('Execution time:', time_taken, 'seconds')\n",
    "    \n",
    "print(\"HKS len\", len(HKS_all_do_task))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f1df4e-6d2d-44f3-8c8a-5317af66fd1a",
   "metadata": {},
   "source": [
    "# obtain eigenvalues for one mesh\n",
    "# input: mesh number referencing a mesh\n",
    "# output: np array of the eigenvalues of that particular mesh\n",
    "def calc_eigvals(num):\n",
    "    \n",
    "    # convert mesh to networkx graph\n",
    "    v, f = igl.read_triangle_mesh(os.path.join(root_folder, \"SHREC11\", \"T\"+str(num)+\".obj\"))\n",
    "    # Mesh in (v,f)\n",
    "    adj_mat = igl.adjacency_matrix(f) # this gets me the adj_mat for this mesh \n",
    "    # print(\"type\", type(adj_mat)) # type <class 'scipy.sparse._csc.csc_matrix'>\n",
    "    \n",
    "    G = nx.from_scipy_sparse_array(adj_mat) # creates a new graph from an adj matrix given as a Scipy sparse array \n",
    "    # print(\"G\", G)\n",
    "    \n",
    "    L = nx.laplacian_matrix(G).toarray() # get Laplacian matrix  \n",
    "    # print(\"L\", L)\n",
    "    \n",
    "    # get eigenvalues from graph Laplacian \n",
    "    eigen_values = np.linalg.eigvals(L)\n",
    "    # print(\"eigen_values: \", eigen_values)\n",
    "    # print(\"eigen_values type: \", type(eigen_values)) # ndarray\n",
    "    # print(\"eigen_values len: \", len(eigen_values)) # 252 nodes hence len is 252 \n",
    "    \n",
    "    return eigen_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e016e1-fb49-4eac-9093-efc242daaae8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# each index of list corresponds to mesh number \n",
    "eig_vals_all_lst = []\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(600):\n",
    "    # obtain eigen values for a mesh\n",
    "    eigen_values = calc_eigvals(i)\n",
    "    # append mesh's eigen values to big list \n",
    "    eig_vals_all_lst.append(eigen_values)\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "time_taken = end_time-start_time\n",
    "\n",
    "print('Execution time to obtain eigen values:', time_taken, 'seconds')\n",
    "\n",
    "# print(\"len of big list\",len(eig_vals_all_lst))\n",
    "# print(\"type of big list\",type(eig_vals_all_lst))\n",
    "# print(\"type of one of the elements\",type(eig_vals_all_lst[0]))\n",
    "# print(eig_vals_all_lst[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d9cc3c-f744-438f-9af1-26a7295c788a",
   "metadata": {},
   "source": [
    "# calculates HKS for one mesh\n",
    "# input: mesh number referencing a mesh\n",
    "# output: HKS for that particular mesh \n",
    "def calc_HKS(num):\n",
    "   \n",
    "    # compute e^t*eigen_value\n",
    "    \n",
    "    HKS = [] \n",
    "    eig_values = eig_vals_all_lst[num]\n",
    "    \n",
    "    \n",
    "    for t in ts:\n",
    "        t_eigen_values = t*eig_values\n",
    "        # print(\"t_eigen_values\", t*eigen_values)\n",
    "\n",
    "        h_t = np.mean(np.exp(-1*eig_values)) # get average to compute h(t) \n",
    "        \n",
    "        # print(\"h_t\", h_t)\n",
    "        \n",
    "        HKS.append(h_t) \n",
    "        \n",
    "    return HKS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dce3cf0-5634-4ac3-9ce3-c79ee220297f",
   "metadata": {},
   "source": [
    "HKS_all = [] \n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(600):\n",
    "    \n",
    "    HKS = calc_HKS(i)\n",
    "    HKS_all.append(HKS)\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "time_taken = end_time-start_time\n",
    "\n",
    "print('Execution time:', time_taken, 'seconds')\n",
    "    \n",
    "print(\"HKS len\", len(HKS_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "4c0e53eb-7cb3-4b9e-afe1-3102a4f5c27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do task <class 'list'>\n",
      "non do task <class 'list'>\n",
      "do task <class 'numpy.ndarray'>\n",
      "non do task <class 'numpy.ndarray'>\n",
      "[[ 0.69503992  0.62205799  0.53578197 ... -0.04242218 -0.04397942\n",
      "  -0.04530437]\n",
      " [ 0.69139735  0.61881573  0.53312126 ... -0.04786033 -0.05054707\n",
      "  -0.05254971]\n",
      " [ 0.69398549  0.62117232  0.53514499 ... -0.04646591 -0.04833545\n",
      "  -0.04985514]\n",
      " ...\n",
      " [ 0.69357913  0.62088386  0.5350269  ... -0.04722566 -0.04901835\n",
      "  -0.05046278]\n",
      " [ 0.69240441  0.61981156  0.53410484 ... -0.04641691 -0.04830271\n",
      "  -0.04997075]\n",
      " [ 0.6910785   0.61847254  0.53274427 ... -0.04751314 -0.05032086\n",
      "  -0.05247441]]\n"
     ]
    }
   ],
   "source": [
    "print(\"do task\", type(HKS_all_do_task))\n",
    "print(\"non do task\", type(HKS_all))\n",
    "# print(\"do task\", HKS_all_do_task[:10])\n",
    "\n",
    "a = np.array(HKS_all_do_task)\n",
    "b = np.array(HKS_all)\n",
    "\n",
    "print(\"do task\", type(a))\n",
    "print(\"non do task\", type(b))\n",
    "\n",
    "print(a-b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "c00e0c05-087f-4915-b2e2-b7176ca89d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I have a list that contains the HKS vectors for each mesh, and I will \n",
    "# convert that into a tensor. The objects numbered 0 through 599 are scrambled\n",
    "# and not organised by category but there is a labels.txt file that groups them\n",
    "# by category, so using that labels.txt file, I will create a list parallel to the\n",
    "# HKS vectors that contains the group number for each mesh, and convert that into a tensor. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "4b676d65-150e-4539-b061-6c6fed218883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# organise labels - taken from Davidson and Richard's code \n",
    "def readLbl(size,fileName):\n",
    "    #takes in file name, returns the labels as an array\n",
    "    file1 = open(fileName, 'r')\n",
    "    Lines = file1.readlines()\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    lbls = np.empty([size])\n",
    "    obj_order_by_grp = np.empty([size]) # list of obj name ordered by grp no.\n",
    "    lbls_order_by_grp = np.empty([size])  # list of grps ordered by grp no.\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        count += 1\n",
    "        text = line.strip()[1:].split('.')\n",
    "        text[1] = text[1].split(' ')[1]\n",
    "        \n",
    "        # list of obj name ordered by grp no.\n",
    "        obj_order_by_grp[count-1] = int(text[0])\n",
    "        \n",
    "        # list of grps ordered by grp no.\n",
    "        lbls_order_by_grp[count-1] = int(text[1])\n",
    "        \n",
    "        # parallel list to HKS - ordered by object no.\n",
    "        lbls[int(text[0])] = int(text[1])\n",
    "        #print(\"Line{}: {}\".format(count, )))\n",
    "        \n",
    "        # file1.close()\n",
    "    return lbls, obj_order_by_grp, lbls_order_by_grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "ae39de89-dc32-40c0-bb58-ce08ea30a967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fName_labels = mesh_dir + 'labels.txt'\n",
    "labels_np, obj_order_by_grp, lbls_order_by_grp = readLbl(NUM_MESHES, path_to_labels)\n",
    "\n",
    "# print(\"obj_order_by_grp\", obj_order_by_grp)\n",
    "# print(\"lbls_order_by_grp\", lbls_order_by_grp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227f8b4a-c6ef-492d-9419-1767c286e4e1",
   "metadata": {},
   "source": [
    "**BINARY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "24eb3152-cc3c-4f83-ac79-6060ec9410ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_binary <class 'numpy.ndarray'>\n",
      "labels_binary [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# obtain labels of group 0 and 1 from larger np.arr, put them in labels_binary\n",
    "# => get a np.arr that is [0,0..0,1,1..1]\n",
    "labels_binary = lbls_order_by_grp[:40] # type: ndarray \n",
    "\n",
    "print(\"labels_binary\", type(labels_binary))\n",
    "print(\"labels_binary\", labels_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd4240f-fc0d-4d90-b1dc-e03e245010bc",
   "metadata": {},
   "source": [
    "**Binary - bigger datatset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "id": "f286bffd-5c29-4725-908f-93f87c75fb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_all=lbls_order_by_grp # type: ndarray \n",
    "\n",
    "# print(\"labels_multi\", type(labels_multi))\n",
    "# print(\"labels_multi\", labels_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52bb749-61ff-4110-8c6a-ca1412b97360",
   "metadata": {},
   "source": [
    "**BINARY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "735cc398-4ceb-4a56-a268-5a0881182217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a np.arr of meshes whose groups are 0 and 1\n",
    "# this is parallel to [0,0..0,1,1..1]\n",
    "obj_order_by_grp_binary = obj_order_by_grp[:40]\n",
    "\n",
    "# convert all floats in np array to int \n",
    "a = obj_order_by_grp_binary.astype(int)\n",
    "\n",
    "# obtain HKS belonging to group 0 and 1\n",
    "HKS_binary_ls = [] \n",
    "for obj in a.tolist(): # tolist() converts type from np.arr to python list \n",
    "    el = HKS_all_do_task[obj]\n",
    "    HKS_binary_ls.append(el)\n",
    "\n",
    "# convert HKS_binary_ls back to np.array \n",
    "HKS_binary = np.array(HKS_binary_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d691389c-f587-4d00-b9f3-82875865bf70",
   "metadata": {},
   "source": [
    "**Binary - bigger datatset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "id": "150cff7c-5f6f-4dd9-bcc6-42c1ea5d7701",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_order_by_grp_all = obj_order_by_grp\n",
    "\n",
    "# convert all floats in np array to int \n",
    "a = obj_order_by_grp_all.astype(int)\n",
    "\n",
    "HKS_all_cat_ls = [] \n",
    "for obj in a.tolist(): # tolist() converts type from np.arr to python list \n",
    "    el = HKS_all_do_task[obj]\n",
    "    HKS_all_cat_ls.append(el)\n",
    "\n",
    "# convert HKS_binary_ls back to np.array \n",
    "HKS_all_cat = np.array(HKS_all_cat_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0995241-c278-4ba3-a20b-8b49f9ecfe1d",
   "metadata": {},
   "source": [
    "**BINARY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5398356a-af93-42e2-8aba-5e595bc0107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"pre shuffle:\", labels_binary)\n",
    "# print(\"pre shuffle:\", HKS_binary)\n",
    "\n",
    "# shuffle two np arrays together\n",
    "rand_indexes = np.arange(len(labels_binary))\n",
    "np.random.shuffle(rand_indexes)\n",
    "labels_binary=labels_binary[rand_indexes]\n",
    "HKS_binary=HKS_binary[rand_indexes]\n",
    "\n",
    "# print(\"post shuffle:\", labels_binary)\n",
    "# print(\"post shuffle:\", HKS_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a103cf61-a93a-4f46-8fb2-daa0b0d3e8f0",
   "metadata": {},
   "source": [
    "**Binary - bigger datatset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "id": "153184af-fda6-42e3-945b-f252d0952ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grps = []\n",
    "HKS_grps = [] # parallel to grps \n",
    "\n",
    "# separates big list into 30 smaller lists \n",
    "for i in range(30):\n",
    "    grps.append(labels_all[i*20:i*20+20])\n",
    "    HKS_grps.append(HKS_all_cat[i*20:i*20+20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "id": "d0246189-3083-40e1-9b57-8c06b37df78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"pre-shuffle\",HKS_grps[13][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "id": "ec872aa3-4667-4b5a-b1ed-852cc17ec3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_all_train_ver2 = []\n",
    "labels_all_test_ver2 = []\n",
    "\n",
    "HKS_all_train_ver2 = []\n",
    "HKS_all_test_ver2 = []\n",
    "\n",
    "# shuffle two np arrays together\n",
    "# then split into train and test set \n",
    "for i in range(30):\n",
    "    # shuffle\n",
    "    rand_indexes = np.arange(20)\n",
    "    np.random.shuffle(rand_indexes)\n",
    "    grps[i]=grps[i][rand_indexes]\n",
    "    HKS_grps[i]=HKS_grps[i][rand_indexes]\n",
    "    \n",
    "    # split into train and test set \n",
    "    labels_train_ver2 = torch.tensor(grps[i][:int(.8*20)]).float()\n",
    "    # torch.reshape(labels_train_ver2, (32,1))\n",
    "    labels_all_train_ver2.append(labels_train_ver2)\n",
    "    \n",
    "    labels_test_ver2 = torch.tensor(grps[i][int(.8*20):]).float()\n",
    "    labels_all_test_ver2.append(labels_test_ver2)\n",
    "    \n",
    "    HKS_train_ver2 = torch.tensor(HKS_grps[i][:int(.8*20)]).float()\n",
    "    HKS_all_train_ver2.append(HKS_train_ver2)\n",
    "    \n",
    "    HKS_test_ver2 = torch.tensor(HKS_grps[i][int(.8*20):]).float()\n",
    "    HKS_all_test_ver2.append(HKS_test_ver2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "id": "5e5a59e3-5f45-49ae-b456-04cd4c38b1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"pre-shuffle\",HKS_grps[13][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "id": "280e92db-bd05-416e-ab4a-8e5448d4408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_all_train_ver2_unsq = labels_all_train_ver2.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "id": "8948ea0d-1772-41ee-aeb9-9cebd952e473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count:  435\n",
      "type <class 'numpy.int32'>\n",
      "type <class 'numpy.int32'>\n",
      "type 0\n",
      "type 1\n"
     ]
    }
   ],
   "source": [
    "# 30 choose 2 -> get every single combination of 2 categories \n",
    "# and append to list\n",
    "size_grps=np.arange(30)\n",
    "lst = []\n",
    "count = 0\n",
    "for i in range(30):\n",
    "    for j in range(i+1, 30):\n",
    "        pointer1 = i\n",
    "        pointer2 = j\n",
    "        x = size_grps[pointer1] # a shld point to array \n",
    "        y = size_grps[pointer2]\n",
    "        count += 1\n",
    "        lst.append((x,y))\n",
    "print(\"count: \", count)\n",
    "\n",
    "mytuple=lst[0]\n",
    "print(\"type\", type(mytuple[0])) # tuple\n",
    "print(\"type\", type(mytuple[1])) # tuple\n",
    "print(\"type\", mytuple[0]) # tuple\n",
    "print(\"type\", mytuple[1]) # tuple\n",
    "\n",
    "# append actual np arrays together\n",
    "\n",
    "labels_v2 = []\n",
    "\n",
    "# goal: combine pairs into one tensor\n",
    "# append tensors to list \n",
    "\n",
    "# obtain tensor based on tuple\n",
    "# combine tensors \n",
    "\n",
    "\n",
    "tensor_list = []\n",
    "for mytuple in lst:\n",
    "    grpA=mytuple[0]\n",
    "    grpB=mytuple[1]\n",
    "    \n",
    "    tensor_pair = torch.cat((labels_all_train_ver2[grpA],labels_all_train_ver2[grpB]))\n",
    "    tensor_list.append(tensor_pair)\n",
    "    \n",
    "    torch.where(grpA)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555f0cfc-87ff-479c-80a5-cc83f00fdf6f",
   "metadata": {},
   "source": [
    "## **BINARY CLASSIFIER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "aa039a7f-8e03-48cb-9c62-d31ab5c8bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training size is hardcoded bc small dataset \n",
    "labels_binary_train = torch.tensor(labels_binary[:int(.8*40)]).float()\n",
    "torch.reshape(labels_binary_train, (32,1))\n",
    "labels_binary_test = torch.tensor(labels_binary[int(.8*40):]).float()\n",
    "\n",
    "HKS_binary_train = torch.tensor(HKS_binary[:int(.8*40)]).float()\n",
    "HKS_binary_test = torch.tensor(HKS_binary[int(.8*40):]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "b7f0e11c-de07-45f6-82b7-155f66157008",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_binary_train_unsqueezed = labels_binary_train.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "a976c3a1-6695-44be-8be2-b376955c286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: replace Sigmoid activations with SiLU, ELU (those can perform better than Sigmoid) \n",
    "# ReLU are also commonly used, they have same problem with Sigmoid \n",
    "\n",
    "# make NN deeper (more layers) and wider (higher dimensions) (layer size) \n",
    "\n",
    "# binary classifier \n",
    "hks_binary_classifier = torch.nn.Sequential( \n",
    "    torch.nn.BatchNorm1d(len(ts)), #  helps NN converge faster \n",
    "    torch.nn.Linear(len(ts), 120), # takes in vector of mine is 4, 16 means weight layer \n",
    "    torch.nn.SiLU(), # activation function, can use sigmoid etc \n",
    "    torch.nn.Linear(120, 120), # takes in vector of mine is 4, 16 means weight layer \n",
    "    torch.nn.SiLU(), # activation function, can use sigmoid etc \n",
    "    torch.nn.Linear(120, 120),\n",
    "    torch.nn.SiLU(), # activation function, can use sigmoid etc \n",
    "    torch.nn.Linear(120, 120),\n",
    "    torch.nn.SiLU(), # activation function, can use sigmoid etc \n",
    "    torch.nn.Linear(120, 120),\n",
    "    torch.nn.SiLU(), # activation function, can use sigmoid etc \n",
    "    torch.nn.Linear(120, 120), # takes in vector of mine is 4, 16 means weight layer \n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(120, 64), # takes in vector of mine is 4, 16 means weight layer \n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(64, 1), #(x,y) -> y is the size of my output I'm doing a linear classifier (binary now, later 25 categories)\n",
    "    # torch.nn.Linear()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "0d274a0d-8d17-42b3-aaf1-3e3536f3c8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "69fd446f-115f-4891-a36d-32f85ab51d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# later: \n",
    "# how to train in batches for PyTorch for 600 (look this up)\n",
    "# in pytorch u create a dataset obj and data loader obj that loads from that dataset\n",
    "# purpose: give u pieces of my dataset at a time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "d8fbdf02-7a07-4443-b684-54de0db8a0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6949, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5020, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0019, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0006, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0003, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0002, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.0001, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(9.2380e-05, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(6.9080e-05, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(5.3461e-05, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(4.2515e-05, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(3.4534e-05, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(2.8537e-05, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(2.3911e-05, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(2.0290e-05, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(1.7389e-05, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(1.5031e-05, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(1.3094e-05, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(1.1485e-05, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(1.0129e-05, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# training loop \n",
    "optimizer = optim.Adam(hks_binary_classifier.parameters(), lr = 0.0001)\n",
    "for i in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    # print(HKS_binary_train)\n",
    "    output = hks_binary_classifier.forward(HKS_binary_train)\n",
    "    # print(output) \n",
    "    loss = criterion(output,labels_binary_train_unsqueezed)\n",
    "    loss.backward()\n",
    "    if i%100 == 0:\n",
    "        print(loss) # print loss every 100 (so not printing every single round) (i mod 30 0) \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3261da-49e7-42d8-89be-d83af0ab5d5a",
   "metadata": {},
   "source": [
    "**Binary w/ bigger dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487687f5-bd32-409c-88c6-ad9f2c33b85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop \n",
    "optimizer = optim.Adam(hks_binary_classifier.parameters(), lr = 0.0001)\n",
    "for i in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    # print(HKS_binary_train)\n",
    "    output = hks_binary_classifier.forward(HKS_binary_train)\n",
    "    # print(output) \n",
    "    loss = criterion(output,labels_binary_train_unsqueezed)\n",
    "    loss.backward()\n",
    "    if i%100 == 0:\n",
    "        print(loss) # print loss every 100 (so not printing every single round) (i mod 30 0) \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "3e5b4fc7-c15a-4c20-b15f-10ac25e57b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# u do the test only once \n",
    "# it outputs predicted values\n",
    "# actual-predicted for each value\n",
    "# get the avg of all the values \n",
    "# hks_binary_classifier.forward(HKS_binary_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "793a10d7-b8db-4f37-a8ea-466129f1e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = hks_binary_classifier.forward(HKS_binary_test)\n",
    "adj_preds = torch.where(preds > 0, 1., 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "d7adaa95-c7d3-431f-91ec-b23ce5a32378",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_binary_test_unsqueezed = labels_binary_test.unsqueeze(1)\n",
    "target = labels_binary_test_unsqueezed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "2e039470-caa0-4a86-a31c-007aec25d38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj_preds tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "target tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "accuracy tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "metric = torchmetrics.classification.BinaryAccuracy()\n",
    "# prediction, target \n",
    "print(\"adj_preds\", adj_preds)\n",
    "print(\"target\", target)\n",
    "metric(adj_preds, target)\n",
    "\n",
    "accuracy = adj_preds-target\n",
    "\n",
    "print(\"accuracy\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5c2885-6bd6-4795-b91d-4d006402f128",
   "metadata": {},
   "source": [
    "## **MULTI CLASS CLASSIFIER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "id": "022ce45c-fde0-4f2f-b200-11b162d7d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast to tensor for all 600 groups\n",
    "# note: everything is scrambled (group wise)\n",
    "# arranged in order from T0-T599 \n",
    "labels_train = torch.tensor(labels_np[:train_size]).float()\n",
    "labels_test = torch.tensor(labels_np[train_size:]).float()\n",
    "\n",
    "HKS_all_train = torch.tensor(HKS_all_do_task[:train_size]).float()\n",
    "HKS_all_test = torch.tensor(HKS_all_do_task[train_size:]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "id": "d75d605d-5d35-48e1-a470-29806c585784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast to int otherwise error message: (one_hot is only applicable to index tensor.)\n",
    "labels_train_one_hot = torch.nn.functional.one_hot(labels_train.to(torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "id": "b2195fe5-4822-466f-b8c2-e661727ce351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_train_one_hot_unsq = labels_train_one_hot.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "id": "2b1ff028-75bf-4947-abf6-982611a04402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-class classifier \n",
    "hks_multi_classifier = torch.nn.Sequential( \n",
    "    torch.nn.BatchNorm1d(len(ts)), #  helps NN converge faster \n",
    "    torch.nn.Linear(len(ts), 150), # takes in vector of mine is 4, 16 means weight layer \n",
    "    torch.nn.SiLU(), # activation function, can use sigmoid etc \n",
    "    torch.nn.Linear(150, 150), # takes in vector of mine is 4, 16 means weight layer \n",
    "    torch.nn.SiLU(), # activation function, can use sigmoid etc \n",
    "    torch.nn.Linear(150, 150),\n",
    "    torch.nn.SiLU(), # activation function, can use sigmoid etc \n",
    "    torch.nn.Linear(150, 150),\n",
    "    torch.nn.SiLU(), # activation function, can use sigmoid etc \n",
    "    torch.nn.Linear(150, 120),\n",
    "    torch.nn.SiLU(), # activation function, can use sigmoid etc \n",
    "    torch.nn.Linear(120, 100), # takes in vector of mine is 4, 16 means weight layer \n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(100, 64), # takes in vector of mine is 4, 16 means weight layer \n",
    "    torch.nn.SiLU(),\n",
    "    torch.nn.Linear(64, 30), #(x,y) -> y is no. of categories \n",
    "    torch.nn.Softmax()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "id": "296d6773-6090-4757-b1a1-82efc8cf4b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "id": "9fda0ec2-f43a-4277-bb2a-2920a98a56e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.6906, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "100 tensor(0.6896, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "200 tensor(0.6896, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "300 tensor(0.6896, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "400 tensor(0.6896, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "500 tensor(0.6896, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "600 tensor(0.6896, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "700 tensor(0.6896, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "800 tensor(0.6896, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "900 tensor(0.6896, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "1000 tensor(0.6896, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "1100 tensor(0.6896, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "1200 tensor(0.6896, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "1300 tensor(0.6896, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "1400 tensor(0.6896, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "1500 tensor(0.6896, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "1600 tensor(0.6899, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "1700 tensor(0.6894, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "1800 tensor(0.6893, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "1900 tensor(0.6893, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Execution time: 59.48548769950867 seconds\n",
      "Execution time: 0.9914247949918111 minutes\n"
     ]
    }
   ],
   "source": [
    "# training loop \n",
    "optimizer = optim.Adam(hks_multi_classifier.parameters(), lr = 0.0002)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    # print(HKS_binary_train)\n",
    "    output = hks_multi_classifier.forward(HKS_all_train)\n",
    "    # print(output) \n",
    "    loss = criterion(output,labels_train_one_hot.float())\n",
    "    loss.backward()\n",
    "    if i%100 == 0:\n",
    "        print(str(i), loss) # print loss every 100 (so not printing every single round) (i mod 30 0) \n",
    "    optimizer.step()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "time_taken = end_time-start_time\n",
    "\n",
    "print('Execution time:', time_taken, 'seconds')\n",
    "print('Execution time:', time_taken/60, 'minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c59c036-fbec-4063-b2ac-6491b06aad07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
