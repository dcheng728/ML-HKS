{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "#read in the data, get normalized adjacency (NxN)\n",
    "%run helpers_preproc.ipynb\n",
    "mesh_dir = 'SHREC11/'\n",
    "\n",
    "label_np = np.array(readLbl(mesh_dir+'labels.txt'))\n",
    "label_mat_np = np.where(igl.all_pairs_distances(label_np,label_np,False) > 0.5,0,1)\n",
    "label_mat = torch.tensor(label_mat_np,requires_grad=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "normed_adjMats_list = []\n",
    "node_sigs_list = []\n",
    "\n",
    "for i in range(600):\n",
    "    fName = 'T' + str(i) + '.obj'\n",
    "    adj_noscale_read = obj_2_adj_noscale(mesh_dir + fName)\n",
    "    adj_scaled_read = obj_2_adj(mesh_dir + fName)\n",
    "\n",
    "    if (adj_scaled_read.shape[0] < 252):\n",
    "        adj_noscale = np.empty([252,252])\n",
    "        adj_scaled = np.empty([252,252])\n",
    "    else:\n",
    "        adj_noscale = adj_noscale_read\n",
    "        adj_scaled = adj_scaled_read\n",
    "\n",
    "    adj_normalized = adj_noscale / np.reshape(np.sum(adj_noscale,axis = 1),[adj_noscale.shape[0],1])\n",
    "\n",
    "    #node level signal extraction\n",
    "    node_degs = np.sum(adj_noscale,axis = 0)\n",
    "    node_neigh_max = np.max(adj_scaled,axis = 0)\n",
    "    node_neigh_min = np.min(adj_scaled,axis = 0)\n",
    "    node_neigh_sum = np.sum(adj_scaled,axis = 0)\n",
    "    node_neigh_mean = np.sum(adj_scaled,axis = 0)\n",
    "\n",
    "    node_sig = np.stack([node_degs,node_neigh_max,node_neigh_min,node_neigh_sum,node_neigh_mean],axis = 1)\n",
    "    node_sigs_list.append(node_sig)\n",
    "\n",
    "    normed_adjMats_list.append(adj_normalized)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "normed_adjMats = np.stack(normed_adjMats_list)\n",
    "node_sigs = np.stack(node_sigs_list)\n",
    "\n",
    "normed_adjMats = torch.tensor(normed_adjMats,requires_grad=False).float()\n",
    "node_sigs = torch.tensor(node_sigs,requires_grad=False).float()\n",
    "\n",
    "normed_adjMats = torch.nan_to_num(normed_adjMats,0,0,0)\n",
    "node_sigs = torch.nan_to_num(node_sigs,0,0,0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[5.0000, 0.0888, 0.0000, 0.3595, 0.3595],\n         [7.0000, 0.1017, 0.0000, 0.4655, 0.4655],\n         [6.0000, 0.0920, 0.0000, 0.4395, 0.4395],\n         ...,\n         [7.0000, 0.1142, 0.0000, 0.5429, 0.5429],\n         [6.0000, 0.1352, 0.0000, 0.4959, 0.4959],\n         [7.0000, 0.1352, 0.0000, 0.5233, 0.5233]],\n\n        [[5.0000, 0.1061, 0.0000, 0.4014, 0.4014],\n         [5.0000, 0.1546, 0.0000, 0.4404, 0.4404],\n         [5.0000, 0.2171, 0.0000, 0.5315, 0.5315],\n         ...,\n         [5.0000, 0.1340, 0.0000, 0.4817, 0.4817],\n         [6.0000, 0.1705, 0.0000, 0.7666, 0.7666],\n         [5.0000, 0.1284, 0.0000, 0.4872, 0.4872]],\n\n        [[4.0000, 0.1228, 0.0000, 0.3313, 0.3313],\n         [5.0000, 0.2132, 0.0000, 0.5203, 0.5203],\n         [4.0000, 0.1201, 0.0000, 0.2880, 0.2880],\n         ...,\n         [5.0000, 0.1041, 0.0000, 0.4604, 0.4604],\n         [5.0000, 0.1094, 0.0000, 0.4256, 0.4256],\n         [4.0000, 0.0913, 0.0000, 0.3470, 0.3470]],\n\n        ...,\n\n        [[5.0000, 0.1554, 0.0000, 0.5991, 0.5991],\n         [7.0000, 0.1458, 0.0000, 0.7658, 0.7658],\n         [6.0000, 0.2105, 0.0000, 0.7404, 0.7404],\n         ...,\n         [6.0000, 0.2476, 0.0000, 0.8704, 0.8704],\n         [4.0000, 0.1443, 0.0000, 0.3843, 0.3843],\n         [4.0000, 0.1277, 0.0000, 0.4273, 0.4273]],\n\n        [[5.0000, 0.1647, 0.0000, 0.6557, 0.6557],\n         [7.0000, 0.1655, 0.0000, 0.9279, 0.9279],\n         [5.0000, 0.1489, 0.0000, 0.6209, 0.6209],\n         ...,\n         [6.0000, 0.1398, 0.0000, 0.7043, 0.7043],\n         [5.0000, 0.1590, 0.0000, 0.5721, 0.5721],\n         [5.0000, 0.1398, 0.0000, 0.5076, 0.5076]],\n\n        [[4.0000, 0.1210, 0.0000, 0.3557, 0.3557],\n         [4.0000, 0.0959, 0.0000, 0.3142, 0.3142],\n         [6.0000, 0.2451, 0.0000, 0.7916, 0.7916],\n         ...,\n         [6.0000, 0.2052, 0.0000, 0.7578, 0.7578],\n         [5.0000, 0.1799, 0.0000, 0.5919, 0.5919],\n         [6.0000, 0.2512, 0.0000, 0.7781, 0.7781]]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_sigs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, widths):\n",
    "        super().__init__()\n",
    "        self.f = 5\n",
    "        self.widths = widths\n",
    "        self.weights = []\n",
    "        self.weights.append(torch.autograd.Variable(torch.rand(size = [self.f,widths[0]])-0.5,requires_grad = True))\n",
    "\n",
    "        for i in range(1,len(self.widths)):\n",
    "            c = self.widths[i]\n",
    "            self.weights.append(torch.autograd.Variable(torch.rand((self.widths[i-1],self.widths[i]))-0.5,requires_grad = True))\n",
    "\n",
    "    def forward(self, adjM,node_sigM):\n",
    "        curr_output = torch.tanh(torch.matmul(torch.matmul(adjM,node_sigM),self.weights[0])) #nxc\n",
    "        for i in range(1,len(self.widths)):\n",
    "            curr_output = torch.tanh(torch.matmul(torch.matmul(adjM,curr_output),self.weights[i])) #nxc\n",
    "\n",
    "        return curr_output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class NNN(torch.nn.Module):\n",
    "    def __init__(self, widths):\n",
    "        super().__init__()\n",
    "        self.input_shape = 2520\n",
    "        self.widths = widths\n",
    "        self.weights = []\n",
    "        self.weights.append(torch.autograd.Variable(torch.rand(size = [self.input_shape,widths[0]])-0.5,requires_grad = True))\n",
    "\n",
    "        for i in range(1,len(self.widths)):\n",
    "            c = self.widths[i]\n",
    "            self.weights.append(torch.autograd.Variable(torch.rand((self.widths[i-1],self.widths[i]))-0.5,requires_grad = True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x,1,2)\n",
    "        torch.matmul(x,self.weights[0])\n",
    "        curr_output = torch.tanh(torch.matmul(x,self.weights[0])) #nxc\n",
    "        for i in range(1,len(self.widths)):\n",
    "            curr_output = torch.tanh(torch.matmul(curr_output,self.weights[i])) #nxc\n",
    "\n",
    "        return curr_output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1551,  0.0792, -0.1879,  ...,  0.2107,  0.1526,  0.1281],\n",
      "         [-0.1552,  0.0793, -0.1878,  ...,  0.2107,  0.1526,  0.1282],\n",
      "         [-0.1538,  0.0763, -0.1872,  ...,  0.2108,  0.1510,  0.1283],\n",
      "         ...,\n",
      "         [-0.1530,  0.0738, -0.1868,  ...,  0.2110,  0.1502,  0.1273],\n",
      "         [-0.1535,  0.0749, -0.1867,  ...,  0.2110,  0.1504,  0.1279],\n",
      "         [-0.1529,  0.0739, -0.1861,  ...,  0.2110,  0.1495,  0.1284]],\n",
      "\n",
      "        [[-0.1609,  0.0888, -0.1975,  ...,  0.2099,  0.1671,  0.1148],\n",
      "         [-0.1640,  0.0960, -0.1989,  ...,  0.2098,  0.1708,  0.1146],\n",
      "         [-0.1616,  0.0909, -0.1976,  ...,  0.2098,  0.1677,  0.1155],\n",
      "         ...,\n",
      "         [-0.1603,  0.0885, -0.1972,  ...,  0.2098,  0.1663,  0.1159],\n",
      "         [-0.1542,  0.0758, -0.1967,  ...,  0.2094,  0.1612,  0.1148],\n",
      "         [-0.1613,  0.0905, -0.1975,  ...,  0.2098,  0.1673,  0.1158]],\n",
      "\n",
      "        [[-0.1613,  0.0902, -0.1962,  ...,  0.2101,  0.1661,  0.1169],\n",
      "         [-0.1612,  0.0897, -0.1960,  ...,  0.2101,  0.1657,  0.1170],\n",
      "         [-0.1614,  0.0903, -0.1961,  ...,  0.2101,  0.1661,  0.1169],\n",
      "         ...,\n",
      "         [-0.1605,  0.0903, -0.1949,  ...,  0.2099,  0.1638,  0.1207],\n",
      "         [-0.1610,  0.0915, -0.1950,  ...,  0.2100,  0.1642,  0.1209],\n",
      "         [-0.1608,  0.0912, -0.1950,  ...,  0.2099,  0.1641,  0.1209]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1615,  0.0912, -0.1984,  ...,  0.2097,  0.1683,  0.1150],\n",
      "         [-0.1596,  0.0872, -0.1983,  ...,  0.2095,  0.1667,  0.1146],\n",
      "         [-0.1605,  0.0888, -0.1986,  ...,  0.2096,  0.1678,  0.1141],\n",
      "         ...,\n",
      "         [-0.1615,  0.0909, -0.1994,  ...,  0.2096,  0.1696,  0.1129],\n",
      "         [-0.1618,  0.0927, -0.1981,  ...,  0.2097,  0.1683,  0.1162],\n",
      "         [-0.1622,  0.0935, -0.1984,  ...,  0.2097,  0.1689,  0.1160]],\n",
      "\n",
      "        [[-0.1630,  0.0950, -0.2010,  ...,  0.2094,  0.1724,  0.1123],\n",
      "         [-0.1606,  0.0897, -0.1997,  ...,  0.2094,  0.1691,  0.1132],\n",
      "         [-0.1626,  0.0941, -0.2007,  ...,  0.2094,  0.1717,  0.1126],\n",
      "         ...,\n",
      "         [-0.1577,  0.0835, -0.2002,  ...,  0.2090,  0.1672,  0.1120],\n",
      "         [-0.1589,  0.0858, -0.2007,  ...,  0.2090,  0.1685,  0.1117],\n",
      "         [-0.1588,  0.0859, -0.2004,  ...,  0.2091,  0.1683,  0.1120]],\n",
      "\n",
      "        [[-0.1638,  0.0953, -0.2000,  ...,  0.2097,  0.1720,  0.1126],\n",
      "         [-0.1627,  0.0936, -0.1968,  ...,  0.2100,  0.1676,  0.1172],\n",
      "         [-0.1632,  0.0933, -0.2010,  ...,  0.2095,  0.1727,  0.1103],\n",
      "         ...,\n",
      "         [-0.1629,  0.0928, -0.2054,  ...,  0.2086,  0.1768,  0.1051],\n",
      "         [-0.1604,  0.0878, -0.1995,  ...,  0.2095,  0.1688,  0.1120],\n",
      "         [-0.1587,  0.0839, -0.1985,  ...,  0.2096,  0.1667,  0.1124]]],\n",
      "       grad_fn=<TanhBackward0>)\n",
      "tensor([[-0.4049, -0.0861,  0.2262,  ...,  0.1662,  0.0654, -0.3846],\n",
      "        [-0.4019, -0.0895,  0.2291,  ...,  0.1804,  0.0596, -0.3933],\n",
      "        [-0.4059, -0.0893,  0.2239,  ...,  0.1690,  0.0584, -0.3884],\n",
      "        ...,\n",
      "        [-0.4057, -0.0899,  0.2240,  ...,  0.1710,  0.0588, -0.3903],\n",
      "        [-0.4048, -0.0912,  0.2247,  ...,  0.1753,  0.0608, -0.3946],\n",
      "        [-0.4045, -0.0910,  0.2252,  ...,  0.1752,  0.0554, -0.3921]],\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "g = GCN([10,10,10,10])\n",
    "n = NNN([10,10,10,10])\n",
    "output1 = g.forward(normed_adjMats,node_sigs)\n",
    "output2 = n.forward(output1)\n",
    "print(output1)\n",
    "print(output2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def lossF(features):\n",
    "    disMat = torch.cdist(features,features)\n",
    "    sameType = disMat * (label_mat-torch.diag(torch.ones(600)))\n",
    "    diffType = disMat * (1-label_mat)\n",
    "    sameTypeMean = torch.sum(sameType) / 12000\n",
    "    diffTypeMean = torch.sum(diffType) /348000\n",
    "\n",
    "    sameTypeStd = torch.sum((sameType - sameTypeMean)**2) / 12000\n",
    "    diffTypeStd = torch.sum((diffType - diffTypeMean)**2) / 348000\n",
    "\n",
    "    #print(disMat)\n",
    "    return  sameTypeMean - diffTypeMean + 0.2 * (sameTypeStd + diffTypeStd)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.4778, grad_fn=<AddBackward0>)\n",
      "tensor(-0.6213, grad_fn=<AddBackward0>)\n",
      "tensor(-2.2288, grad_fn=<AddBackward0>)\n",
      "tensor(-2.1794, grad_fn=<AddBackward0>)\n",
      "tensor(-1.6904, grad_fn=<AddBackward0>)\n",
      "tensor(-1.7257, grad_fn=<AddBackward0>)\n",
      "tensor(-2.0248, grad_fn=<AddBackward0>)\n",
      "tensor(-2.2867, grad_fn=<AddBackward0>)\n",
      "tensor(-2.3498, grad_fn=<AddBackward0>)\n",
      "tensor(-2.1862, grad_fn=<AddBackward0>)\n",
      "tensor(-2.1077, grad_fn=<AddBackward0>)\n",
      "tensor(-2.1888, grad_fn=<AddBackward0>)\n",
      "tensor(-2.3151, grad_fn=<AddBackward0>)\n",
      "tensor(-2.3666, grad_fn=<AddBackward0>)\n",
      "tensor(-2.3409, grad_fn=<AddBackward0>)\n",
      "tensor(-2.3131, grad_fn=<AddBackward0>)\n",
      "tensor(-2.3111, grad_fn=<AddBackward0>)\n",
      "tensor(-2.3276, grad_fn=<AddBackward0>)\n",
      "tensor(-2.3593, grad_fn=<AddBackward0>)\n",
      "tensor(-2.3776, grad_fn=<AddBackward0>)\n",
      "tensor(-2.3840, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4118, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4344, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4067, grad_fn=<AddBackward0>)\n",
      "tensor(-2.3781, grad_fn=<AddBackward0>)\n",
      "tensor(-2.3905, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4362, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4651, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4448, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4282, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4279, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4351, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4431, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4568, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4688, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4595, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4436, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4467, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4664, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4718, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4653, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4651, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4651, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4668, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4749, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4724, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4694, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4770, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4733, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4736, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4783, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4774, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4789, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4757, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4798, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4814, grad_fn=<AddBackward0>)\n",
      "tensor(-2.4786, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [60]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m10000\u001B[39m):\n\u001B[1;32m      3\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m----> 4\u001B[0m     output \u001B[38;5;241m=\u001B[39m n\u001B[38;5;241m.\u001B[39mforward(\u001B[43mg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnormed_adjMats\u001B[49m\u001B[43m,\u001B[49m\u001B[43mnode_sigs\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m      5\u001B[0m     loss \u001B[38;5;241m=\u001B[39m lossF(output)\n\u001B[1;32m      6\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36mGCN.forward\u001B[0;34m(self, adjM, node_sigM)\u001B[0m\n\u001B[1;32m     14\u001B[0m curr_output \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtanh(torch\u001B[38;5;241m.\u001B[39mmatmul(torch\u001B[38;5;241m.\u001B[39mmatmul(adjM,node_sigM),\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights[\u001B[38;5;241m0\u001B[39m])) \u001B[38;5;66;03m#nxc\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwidths)):\n\u001B[0;32m---> 16\u001B[0m     curr_output \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtanh(torch\u001B[38;5;241m.\u001B[39mmatmul(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatmul\u001B[49m\u001B[43m(\u001B[49m\u001B[43madjM\u001B[49m\u001B[43m,\u001B[49m\u001B[43mcurr_output\u001B[49m\u001B[43m)\u001B[49m,\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights[i])) \u001B[38;5;66;03m#nxc\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m curr_output\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(g.weights + n.weights,lr = 0.0001)\n",
    "for i in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    output = n.forward(g.forward(normed_adjMats,node_sigs))\n",
    "    loss = lossF(output)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t4/vdzymfw562g8ml8ks_2b5l5r0000gn/T/ipykernel_25938/401933162.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask_same = torch.tensor(label_mat - torch.diag(torch.ones(600)),dtype=bool).flatten()\n",
      "/var/folders/t4/vdzymfw562g8ml8ks_2b5l5r0000gn/T/ipykernel_25938/401933162.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask_diff = torch.tensor(1 - label_mat,dtype=bool).flatten()\n"
     ]
    }
   ],
   "source": [
    "disMat = torch.cdist(output,output).flatten()\n",
    "mask_same = torch.tensor(label_mat - torch.diag(torch.ones(600)),dtype=bool).flatten()\n",
    "mask_diff = torch.tensor(1 - label_mat,dtype=bool).flatten()\n",
    "sameComp = disMat[mask_same].detach().numpy()\n",
    "diffComp = disMat[mask_diff].detach().numpy()\n",
    "weightsSame = np.ones_like(sameComp) / len(sameComp)\n",
    "weightsDiff = np.ones_like(diffComp) / len(diffComp)\n",
    "bins = torch.arange(0,0.1,0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([0.0533334 , 0.03736727, 0.03491089, 0.03280845, 0.03403658,\n        0.0329839 , 0.03526287, 0.02859288, 0.03052247, 0.02508456,\n        0.02789122, 0.02368122, 0.02280414, 0.02420747, 0.02017289,\n        0.01964664, 0.02140239, 0.0161438 , 0.0161438 , 0.01772308,\n        0.01351166, 0.01509094, 0.01228333, 0.01210785, 0.01263428,\n        0.00947571, 0.0087738 , 0.00859833, 0.0080719 , 0.0080719 ,\n        0.00754547, 0.00614166, 0.00772095, 0.00543976, 0.00737   ,\n        0.00596619, 0.00649261, 0.00701904, 0.00526428, 0.00508881,\n        0.00350952, 0.00579071, 0.00526428, 0.00543976, 0.00473785,\n        0.00403595, 0.00473785, 0.00561523, 0.00491333, 0.00508881,\n        0.00386047, 0.00526428, 0.003685  , 0.003685  , 0.00386047,\n        0.00526428, 0.00403595, 0.00350952, 0.00280762, 0.00333405,\n        0.00228119, 0.00228119, 0.00263214, 0.00193024, 0.00263214,\n        0.00298309, 0.00175476, 0.00122833, 0.00263214, 0.00157928,\n        0.00193024, 0.00087738, 0.00175476, 0.00105286, 0.00105286,\n        0.00157928, 0.00105286, 0.00087738, 0.00122833, 0.00140381,\n        0.0007019 , 0.00087738, 0.00122833, 0.00052643, 0.00122833,\n        0.00035095, 0.00105286, 0.00105286, 0.00087738, 0.00122833,\n        0.00122833, 0.0007019 , 0.00122833, 0.00140381, 0.00157928,\n        0.00105286, 0.00122833, 0.00052643, 0.00157928, 0.00017548,\n        0.00087738, 0.00105286, 0.00052643, 0.00140381, 0.0007019 ,\n        0.00087738, 0.00035095, 0.0007019 , 0.00035095, 0.00052643,\n        0.00052643, 0.00035095, 0.00105286, 0.00035095, 0.00140381,\n        0.0007019 , 0.00035095, 0.00122833, 0.00122833, 0.00052643,\n        0.00052643, 0.0007019 , 0.00035095, 0.00052643, 0.00017548,\n        0.00052643, 0.0007019 , 0.0007019 , 0.00087738, 0.0007019 ,\n        0.0007019 , 0.00052643, 0.00035095, 0.00087738, 0.00017548,\n        0.00087738, 0.00052643, 0.00052643, 0.00105286, 0.0007019 ,\n        0.00017548, 0.00035095, 0.00017548, 0.0007019 , 0.00052643,\n        0.0007019 , 0.00035095, 0.00017548, 0.00035095, 0.00017548,\n        0.        , 0.00052643, 0.00035095, 0.00035095, 0.00017548,\n        0.00017548, 0.00052643, 0.00017548, 0.0007019 , 0.00052643,\n        0.00035095, 0.0007019 , 0.00105286, 0.00052643, 0.00017548,\n        0.00035095, 0.00035095, 0.00052643, 0.        , 0.00052643,\n        0.00017548, 0.00052643, 0.00052643, 0.00035095, 0.0007019 ,\n        0.00017548, 0.00035095, 0.00017548, 0.00035095, 0.00087738,\n        0.0007019 , 0.00017548, 0.00052643, 0.00017548, 0.00105286,\n        0.        , 0.00017548, 0.00017548, 0.        , 0.00017548,\n        0.        , 0.0007019 , 0.        , 0.        , 0.00035095,\n        0.00035095, 0.        , 0.00017548, 0.00017548, 0.00017548,\n        0.00017548, 0.00017548, 0.00087738, 0.00035095, 0.00035095,\n        0.00035095, 0.00052643, 0.00052643, 0.00017548, 0.00035095,\n        0.00017548, 0.00035095, 0.00052643, 0.00052643, 0.00017548,\n        0.00052643, 0.00017548, 0.00105286, 0.        , 0.        ,\n        0.00017548, 0.00017548, 0.00017548, 0.        , 0.00017548,\n        0.00017548, 0.00017548, 0.00017548, 0.00035095, 0.00052643,\n        0.        , 0.        , 0.00017548, 0.00017548, 0.00017548,\n        0.00052643, 0.00017548, 0.00017548, 0.        , 0.00052643,\n        0.0007019 , 0.00052643, 0.00035095, 0.00052643, 0.00052643,\n        0.00052643, 0.00052643, 0.00017548, 0.00052643, 0.        ,\n        0.        , 0.00017548, 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.00017548, 0.00052643, 0.00035095,\n        0.        , 0.00017548, 0.00035095, 0.00052643, 0.00035095,\n        0.00035095, 0.00035095, 0.00017548, 0.        , 0.00017548,\n        0.        , 0.00017548, 0.00035095, 0.        , 0.        ,\n        0.        , 0.00017548, 0.00035095, 0.00017548, 0.00017548,\n        0.00035095, 0.00035095, 0.        , 0.        , 0.        ,\n        0.        , 0.00035095, 0.        , 0.00035095, 0.00017548,\n        0.        , 0.00017548, 0.        , 0.        , 0.00017548,\n        0.        , 0.00017548, 0.00035095, 0.00017548, 0.00017548,\n        0.        , 0.00017548, 0.        , 0.00035095, 0.        ,\n        0.00017548, 0.00017548, 0.00017548, 0.0007019 , 0.00035095,\n        0.00035095, 0.        , 0.        , 0.00052643, 0.00017548,\n        0.00017548, 0.00035095, 0.00035095, 0.00017548, 0.        ,\n        0.        , 0.00035095, 0.        , 0.        , 0.        ,\n        0.00017548, 0.        , 0.00035095, 0.00017548, 0.00035095,\n        0.00017548, 0.00017548, 0.        , 0.00035095, 0.        ,\n        0.00052643, 0.00035095, 0.0007019 , 0.00052643, 0.00017548,\n        0.00017548, 0.00052643, 0.00052643, 0.00017548, 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.00017548, 0.        ,\n        0.        , 0.00017548, 0.00017548, 0.        , 0.00017548,\n        0.00017548, 0.00017548, 0.00035095, 0.        , 0.00052643,\n        0.00017548, 0.00017548, 0.        , 0.00035095, 0.00017548,\n        0.00017548, 0.00017548, 0.00035095, 0.00017548, 0.        ,\n        0.00017548, 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.00017548,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.00017548, 0.        , 0.        , 0.00017548, 0.        ,\n        0.        , 0.00052643, 0.00017548, 0.00017548, 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.00035095,\n        0.        , 0.00017548, 0.00017548, 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.00017548, 0.        , 0.00017548, 0.        , 0.        ,\n        0.00017548, 0.        , 0.00035095, 0.00017548, 0.00017548,\n        0.        , 0.00017548, 0.00017548, 0.        , 0.00017548,\n        0.00052643, 0.00035095, 0.        , 0.00035095, 0.        ,\n        0.00017548, 0.00017548, 0.00017548, 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.00017548, 0.        , 0.        , 0.00035095,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.00017548, 0.        , 0.        , 0.        ,\n        0.00017548, 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.00035095, 0.        , 0.00017548,\n        0.        , 0.00017548, 0.00017548, 0.00017548, 0.        ,\n        0.        , 0.00017548, 0.00052643, 0.00035095, 0.00052643,\n        0.00017548, 0.00052643, 0.00052643, 0.00052643, 0.00035095,\n        0.00035095, 0.0007019 , 0.0007019 , 0.0007019 , 0.00052643,\n        0.0007019 , 0.00035095, 0.00017548, 0.00017548, 0.00035095,\n        0.00035095, 0.00017548, 0.00017548, 0.00017548, 0.00035095,\n        0.00017548, 0.00087738, 0.        , 0.00017548, 0.00017548,\n        0.00017548, 0.        , 0.00017548, 0.        , 0.00035095,\n        0.00035095, 0.00017548, 0.00035095, 0.        , 0.        ,\n        0.        , 0.        , 0.00035095, 0.00017548, 0.00035095,\n        0.00017548, 0.00017548, 0.00017548, 0.00017548, 0.00017548,\n        0.00017548, 0.        , 0.00017548, 0.        , 0.00017548,\n        0.        , 0.        , 0.00017548, 0.00017548, 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.00017548,\n        0.        , 0.00017548, 0.        , 0.00017548, 0.        ,\n        0.        , 0.00017548, 0.        , 0.00035095, 0.00017548,\n        0.00052643, 0.00017548, 0.        , 0.00035095, 0.00017548,\n        0.00017548, 0.00035095, 0.        , 0.00017548, 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.00017548, 0.        , 0.        , 0.        , 0.00017548,\n        0.        , 0.        , 0.        , 0.        , 0.00035095,\n        0.        , 0.        , 0.        , 0.00017548, 0.00017548,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.00017548, 0.        , 0.00017548, 0.00017548, 0.00035095,\n        0.        , 0.        , 0.        , 0.00052643, 0.00035095,\n        0.00035095, 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.00017548,\n        0.00017548, 0.        , 0.00017548, 0.        , 0.00017548,\n        0.        , 0.        , 0.        , 0.00017548, 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.00017548,\n        0.        , 0.        , 0.00017548, 0.        , 0.        ,\n        0.        , 0.        , 0.00017548, 0.00017548, 0.00017548,\n        0.00017548, 0.00017548, 0.00017548, 0.        , 0.00035095,\n        0.00017548, 0.        , 0.        , 0.        , 0.        ,\n        0.00017548, 0.00017548, 0.        , 0.00017548, 0.        ,\n        0.        , 0.        , 0.        , 0.00017548, 0.        ,\n        0.        , 0.00017548, 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.00017548, 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.00017548, 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.00017548, 0.        , 0.        ,\n        0.00017548, 0.00017548, 0.00017548, 0.00017548, 0.        ,\n        0.        , 0.        , 0.        , 0.00017548, 0.        ,\n        0.00017548, 0.        , 0.        , 0.        , 0.00017548,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.00017548, 0.        , 0.00017548, 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.00017548, 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.00017548, 0.        , 0.        , 0.        ,\n        0.00017548, 0.        , 0.00017548, 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.00017548, 0.        , 0.00017548,\n        0.00017548, 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.00035095, 0.00017548, 0.        , 0.        ,\n        0.        , 0.00017548, 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.00017548, 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.00017548, 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.00017548, 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.00017548,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        ]),\n array([0.   , 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008,\n        0.009, 0.01 , 0.011, 0.012, 0.013, 0.014, 0.015, 0.016, 0.017,\n        0.018, 0.019, 0.02 , 0.021, 0.022, 0.023, 0.024, 0.025, 0.026,\n        0.027, 0.028, 0.029, 0.03 , 0.031, 0.032, 0.033, 0.034, 0.035,\n        0.036, 0.037, 0.038, 0.039, 0.04 , 0.041, 0.042, 0.043, 0.044,\n        0.045, 0.046, 0.047, 0.048, 0.049, 0.05 , 0.051, 0.052, 0.053,\n        0.054, 0.055, 0.056, 0.057, 0.058, 0.059, 0.06 , 0.061, 0.062,\n        0.063, 0.064, 0.065, 0.066, 0.067, 0.068, 0.069, 0.07 , 0.071,\n        0.072, 0.073, 0.074, 0.075, 0.076, 0.077, 0.078, 0.079, 0.08 ,\n        0.081, 0.082, 0.083, 0.084, 0.085, 0.086, 0.087, 0.088, 0.089,\n        0.09 , 0.091, 0.092, 0.093, 0.094, 0.095, 0.096, 0.097, 0.098,\n        0.099, 0.1  , 0.101, 0.102, 0.103, 0.104, 0.105, 0.106, 0.107,\n        0.108, 0.109, 0.11 , 0.111, 0.112, 0.113, 0.114, 0.115, 0.116,\n        0.117, 0.118, 0.119, 0.12 , 0.121, 0.122, 0.123, 0.124, 0.125,\n        0.126, 0.127, 0.128, 0.129, 0.13 , 0.131, 0.132, 0.133, 0.134,\n        0.135, 0.136, 0.137, 0.138, 0.139, 0.14 , 0.141, 0.142, 0.143,\n        0.144, 0.145, 0.146, 0.147, 0.148, 0.149, 0.15 , 0.151, 0.152,\n        0.153, 0.154, 0.155, 0.156, 0.157, 0.158, 0.159, 0.16 , 0.161,\n        0.162, 0.163, 0.164, 0.165, 0.166, 0.167, 0.168, 0.169, 0.17 ,\n        0.171, 0.172, 0.173, 0.174, 0.175, 0.176, 0.177, 0.178, 0.179,\n        0.18 , 0.181, 0.182, 0.183, 0.184, 0.185, 0.186, 0.187, 0.188,\n        0.189, 0.19 , 0.191, 0.192, 0.193, 0.194, 0.195, 0.196, 0.197,\n        0.198, 0.199, 0.2  , 0.201, 0.202, 0.203, 0.204, 0.205, 0.206,\n        0.207, 0.208, 0.209, 0.21 , 0.211, 0.212, 0.213, 0.214, 0.215,\n        0.216, 0.217, 0.218, 0.219, 0.22 , 0.221, 0.222, 0.223, 0.224,\n        0.225, 0.226, 0.227, 0.228, 0.229, 0.23 , 0.231, 0.232, 0.233,\n        0.234, 0.235, 0.236, 0.237, 0.238, 0.239, 0.24 , 0.241, 0.242,\n        0.243, 0.244, 0.245, 0.246, 0.247, 0.248, 0.249, 0.25 , 0.251,\n        0.252, 0.253, 0.254, 0.255, 0.256, 0.257, 0.258, 0.259, 0.26 ,\n        0.261, 0.262, 0.263, 0.264, 0.265, 0.266, 0.267, 0.268, 0.269,\n        0.27 , 0.271, 0.272, 0.273, 0.274, 0.275, 0.276, 0.277, 0.278,\n        0.279, 0.28 , 0.281, 0.282, 0.283, 0.284, 0.285, 0.286, 0.287,\n        0.288, 0.289, 0.29 , 0.291, 0.292, 0.293, 0.294, 0.295, 0.296,\n        0.297, 0.298, 0.299, 0.3  , 0.301, 0.302, 0.303, 0.304, 0.305,\n        0.306, 0.307, 0.308, 0.309, 0.31 , 0.311, 0.312, 0.313, 0.314,\n        0.315, 0.316, 0.317, 0.318, 0.319, 0.32 , 0.321, 0.322, 0.323,\n        0.324, 0.325, 0.326, 0.327, 0.328, 0.329, 0.33 , 0.331, 0.332,\n        0.333, 0.334, 0.335, 0.336, 0.337, 0.338, 0.339, 0.34 , 0.341,\n        0.342, 0.343, 0.344, 0.345, 0.346, 0.347, 0.348, 0.349, 0.35 ,\n        0.351, 0.352, 0.353, 0.354, 0.355, 0.356, 0.357, 0.358, 0.359,\n        0.36 , 0.361, 0.362, 0.363, 0.364, 0.365, 0.366, 0.367, 0.368,\n        0.369, 0.37 , 0.371, 0.372, 0.373, 0.374, 0.375, 0.376, 0.377,\n        0.378, 0.379, 0.38 , 0.381, 0.382, 0.383, 0.384, 0.385, 0.386,\n        0.387, 0.388, 0.389, 0.39 , 0.391, 0.392, 0.393, 0.394, 0.395,\n        0.396, 0.397, 0.398, 0.399, 0.4  , 0.401, 0.402, 0.403, 0.404,\n        0.405, 0.406, 0.407, 0.408, 0.409, 0.41 , 0.411, 0.412, 0.413,\n        0.414, 0.415, 0.416, 0.417, 0.418, 0.419, 0.42 , 0.421, 0.422,\n        0.423, 0.424, 0.425, 0.426, 0.427, 0.428, 0.429, 0.43 , 0.431,\n        0.432, 0.433, 0.434, 0.435, 0.436, 0.437, 0.438, 0.439, 0.44 ,\n        0.441, 0.442, 0.443, 0.444, 0.445, 0.446, 0.447, 0.448, 0.449,\n        0.45 , 0.451, 0.452, 0.453, 0.454, 0.455, 0.456, 0.457, 0.458,\n        0.459, 0.46 , 0.461, 0.462, 0.463, 0.464, 0.465, 0.466, 0.467,\n        0.468, 0.469, 0.47 , 0.471, 0.472, 0.473, 0.474, 0.475, 0.476,\n        0.477, 0.478, 0.479, 0.48 , 0.481, 0.482, 0.483, 0.484, 0.485,\n        0.486, 0.487, 0.488, 0.489, 0.49 , 0.491, 0.492, 0.493, 0.494,\n        0.495, 0.496, 0.497, 0.498, 0.499, 0.5  , 0.501, 0.502, 0.503,\n        0.504, 0.505, 0.506, 0.507, 0.508, 0.509, 0.51 , 0.511, 0.512,\n        0.513, 0.514, 0.515, 0.516, 0.517, 0.518, 0.519, 0.52 , 0.521,\n        0.522, 0.523, 0.524, 0.525, 0.526, 0.527, 0.528, 0.529, 0.53 ,\n        0.531, 0.532, 0.533, 0.534, 0.535, 0.536, 0.537, 0.538, 0.539,\n        0.54 , 0.541, 0.542, 0.543, 0.544, 0.545, 0.546, 0.547, 0.548,\n        0.549, 0.55 , 0.551, 0.552, 0.553, 0.554, 0.555, 0.556, 0.557,\n        0.558, 0.559, 0.56 , 0.561, 0.562, 0.563, 0.564, 0.565, 0.566,\n        0.567, 0.568, 0.569, 0.57 , 0.571, 0.572, 0.573, 0.574, 0.575,\n        0.576, 0.577, 0.578, 0.579, 0.58 , 0.581, 0.582, 0.583, 0.584,\n        0.585, 0.586, 0.587, 0.588, 0.589, 0.59 , 0.591, 0.592, 0.593,\n        0.594, 0.595, 0.596, 0.597, 0.598, 0.599, 0.6  , 0.601, 0.602,\n        0.603, 0.604, 0.605, 0.606, 0.607, 0.608, 0.609, 0.61 , 0.611,\n        0.612, 0.613, 0.614, 0.615, 0.616, 0.617, 0.618, 0.619, 0.62 ,\n        0.621, 0.622, 0.623, 0.624, 0.625, 0.626, 0.627, 0.628, 0.629,\n        0.63 , 0.631, 0.632, 0.633, 0.634, 0.635, 0.636, 0.637, 0.638,\n        0.639, 0.64 , 0.641, 0.642, 0.643, 0.644, 0.645, 0.646, 0.647,\n        0.648, 0.649, 0.65 , 0.651, 0.652, 0.653, 0.654, 0.655, 0.656,\n        0.657, 0.658, 0.659, 0.66 , 0.661, 0.662, 0.663, 0.664, 0.665,\n        0.666, 0.667, 0.668, 0.669, 0.67 , 0.671, 0.672, 0.673, 0.674,\n        0.675, 0.676, 0.677, 0.678, 0.679, 0.68 , 0.681, 0.682, 0.683,\n        0.684, 0.685, 0.686, 0.687, 0.688, 0.689, 0.69 , 0.691, 0.692,\n        0.693, 0.694, 0.695, 0.696, 0.697, 0.698, 0.699, 0.7  , 0.701,\n        0.702, 0.703, 0.704, 0.705, 0.706, 0.707, 0.708, 0.709, 0.71 ,\n        0.711, 0.712, 0.713, 0.714, 0.715, 0.716, 0.717, 0.718, 0.719,\n        0.72 , 0.721, 0.722, 0.723, 0.724, 0.725, 0.726, 0.727, 0.728,\n        0.729, 0.73 , 0.731, 0.732, 0.733, 0.734, 0.735, 0.736, 0.737,\n        0.738, 0.739, 0.74 , 0.741, 0.742, 0.743, 0.744, 0.745, 0.746,\n        0.747, 0.748, 0.749, 0.75 , 0.751, 0.752, 0.753, 0.754, 0.755,\n        0.756, 0.757, 0.758, 0.759, 0.76 , 0.761, 0.762, 0.763, 0.764,\n        0.765, 0.766, 0.767, 0.768, 0.769, 0.77 , 0.771, 0.772, 0.773,\n        0.774, 0.775, 0.776, 0.777, 0.778, 0.779, 0.78 , 0.781, 0.782,\n        0.783, 0.784, 0.785, 0.786, 0.787, 0.788, 0.789, 0.79 , 0.791,\n        0.792, 0.793, 0.794, 0.795, 0.796, 0.797, 0.798, 0.799, 0.8  ,\n        0.801, 0.802, 0.803, 0.804, 0.805, 0.806, 0.807, 0.808, 0.809,\n        0.81 , 0.811, 0.812, 0.813, 0.814, 0.815, 0.816, 0.817, 0.818,\n        0.819, 0.82 , 0.821, 0.822, 0.823, 0.824, 0.825, 0.826, 0.827,\n        0.828, 0.829, 0.83 , 0.831, 0.832, 0.833, 0.834, 0.835, 0.836,\n        0.837, 0.838, 0.839, 0.84 , 0.841, 0.842, 0.843, 0.844, 0.845,\n        0.846, 0.847, 0.848, 0.849, 0.85 , 0.851, 0.852, 0.853, 0.854,\n        0.855, 0.856, 0.857, 0.858, 0.859, 0.86 , 0.861, 0.862, 0.863,\n        0.864, 0.865, 0.866, 0.867, 0.868, 0.869, 0.87 , 0.871, 0.872,\n        0.873, 0.874, 0.875, 0.876, 0.877, 0.878, 0.879, 0.88 , 0.881,\n        0.882, 0.883, 0.884, 0.885, 0.886, 0.887, 0.888, 0.889, 0.89 ,\n        0.891, 0.892, 0.893, 0.894, 0.895, 0.896, 0.897, 0.898, 0.899,\n        0.9  , 0.901, 0.902, 0.903, 0.904, 0.905, 0.906, 0.907, 0.908,\n        0.909, 0.91 , 0.911, 0.912, 0.913, 0.914, 0.915, 0.916, 0.917,\n        0.918, 0.919, 0.92 , 0.921, 0.922, 0.923, 0.924, 0.925, 0.926,\n        0.927, 0.928, 0.929, 0.93 , 0.931, 0.932, 0.933, 0.934, 0.935,\n        0.936, 0.937, 0.938, 0.939, 0.94 , 0.941, 0.942, 0.943, 0.944,\n        0.945, 0.946, 0.947, 0.948, 0.949, 0.95 , 0.951, 0.952, 0.953,\n        0.954, 0.955, 0.956, 0.957, 0.958, 0.959, 0.96 , 0.961, 0.962,\n        0.963, 0.964, 0.965, 0.966, 0.967, 0.968, 0.969, 0.97 , 0.971,\n        0.972, 0.973, 0.974, 0.975, 0.976, 0.977, 0.978, 0.979, 0.98 ,\n        0.981, 0.982, 0.983, 0.984, 0.985, 0.986, 0.987, 0.988, 0.989,\n        0.99 , 0.991, 0.992, 0.993, 0.994, 0.995, 0.996, 0.997, 0.998,\n        0.999]),\n <BarContainer object of 999 artists>)"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sameComp,bins=np.arange(0,1,0.001),weights=weightsSame,log = False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([0.00534483, 0.00328736, 0.00268391, ..., 0.        , 0.        ,\n        0.        ]),\n array([0.000e+00, 1.000e-03, 2.000e-03, ..., 7.997e+00, 7.998e+00,\n        7.999e+00]),\n <BarContainer object of 7999 artists>)"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAR2UlEQVR4nO3df6xkZ13H8feHXSq0SorpBcvuxq24IdkQUjY3pUhCVES6hXRFQ9ImWlJNlibUgMbgIglijLGKiqk2rRWqNhYaBBo3dLUQlKB/FHu3lEJti9da6NIFLjEWsSZ15esf9yxch7kzZ+6duzN3n/crmew95zzPme8zvz5znjkzm6pCktSeZ8y6AEnSbBgAktQoA0CSGmUASFKjDABJatTOWRcwiQsuuKD27t076zIkaVs5fvz416tqYXD9tgqAvXv3srS0NOsyJGlbSfLFYeudApKkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIa1SsAklyW5JEky0mODNmeJDd02x9IcmBc3yTvSvLlJPd3l8unMyRJUh9jAyDJDuBG4CCwH7gqyf6BZgeBfd3lMHBTz77vqaqLu8uxzQ5GktRfnyOAS4Dlqnq0qp4G7gAODbQ5BNxWq+4Bzk9yYc++kqQZ6BMAu4DH1yyf6Nb1aTOu73XdlNGtSZ477MqTHE6ylGRpZWWlR7mSpD76BECGrKuebUb1vQl4IXAxcBL4/WFXXlW3VNViVS0uLCz0KFeS1MfOHm1OAHvWLO8GnujZ5pz1+lbVV0+vTPKnwEd7Vy1J2rQ+RwD3AvuSXJTkHOBK4OhAm6PA1d3ZQJcCT1bVyVF9u88ITns98PlNjkWSNIGxRwBVdSrJdcDdwA7g1qp6MMm13fabgWPA5cAy8BRwzai+3a5/N8nFrE4JPQa8aYrjkiSNkarB6fz5tbi4WEtLS7MuQ5K2lSTHq2pxcL3fBJakRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmN6hUASS5L8kiS5SRHhmxPkhu67Q8kOTBB319JUkku2NxQJEmTGBsASXYANwIHgf3AVUn2DzQ7COzrLoeBm/r0TbIHeDXwpU2PRJI0kT5HAJcAy1X1aFU9DdwBHBpocwi4rVbdA5yf5MIefd8DvA2ozQ5EkjSZPgGwC3h8zfKJbl2fNuv2TXIF8OWq+uyoK09yOMlSkqWVlZUe5UqS+ugTABmybvAd+3pthq5Pci7wDuCd4668qm6pqsWqWlxYWBhbrCSpnz4BcALYs2Z5N/BEzzbrrX8hcBHw2SSPdevvS/IDkxQvSdq4PgFwL7AvyUVJzgGuBI4OtDkKXN2dDXQp8GRVnVyvb1V9rqqeV1V7q2ovq0FxoKq+Mq2BSZJG2zmuQVWdSnIdcDewA7i1qh5Mcm23/WbgGHA5sAw8BVwzqu+WjESSNJFUbZ8TcBYXF2tpaWnWZUjStpLkeFUtDq73m8CS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY3qFQBJLkvySJLlJEeGbE+SG7rtDyQ5MK5vkt/s2t6f5GNJXjCdIUmS+hgbAEl2ADcCB4H9wFVJ9g80Owjs6y6HgZt69H13Vb2kqi4GPgq8c9OjkST11ucI4BJguaoeraqngTuAQwNtDgG31ap7gPOTXDiqb1V9Y03/84Da5FgkSRPY2aPNLuDxNcsngJf1aLNrXN8kvwVcDTwJ/FjvqiVJm9bnCCBD1g2+W1+vzci+VfWOqtoD3A5cN/TKk8NJlpIsrays9ChXktRHnwA4AexZs7wbeKJnmz59Ad4P/MywK6+qW6pqsaoWFxYWepQrSeqjTwDcC+xLclGSc4ArgaMDbY4CV3dnA10KPFlVJ0f1TbJvTf8rgIc3ORZJ0gTGfgZQVaeSXAfcDewAbq2qB5Nc222/GTgGXA4sA08B14zq2+36+iQvAr4FfBG4dqojkySNlKrtc/LN4uJiLS0tzboMSdpWkhyvqsXB9X4TWJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAkube3iN3zbqEs5IBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGtUrAJJcluSRJMtJjgzZniQ3dNsfSHJgXN8k707ycNf+ziTnT2VEkqRexgZAkh3AjcBBYD9wVZL9A80OAvu6y2Hgph59Pw68uKpeAnwBePumRyNJ6q3PEcAlwHJVPVpVTwN3AIcG2hwCbqtV9wDnJ7lwVN+q+lhVner63wPsnsJ4JEk99QmAXcDja5ZPdOv6tOnTF+Dngb8ZduVJDidZSrK0srLSo1xJUh99AiBD1lXPNmP7JnkHcAq4fdiVV9UtVbVYVYsLCws9ypUk9bGzR5sTwJ41y7uBJ3q2OWdU3yRvBF4HvKqqBkNFkrSF+hwB3AvsS3JRknOAK4GjA22OAld3ZwNdCjxZVSdH9U1yGfCrwBVV9dSUxiNJ6mnsEUBVnUpyHXA3sAO4taoeTHJtt/1m4BhwObAMPAVcM6pvt+s/Br4H+HgSgHuq6tppDk6StL4+U0BU1TFWX+TXrrt5zd8FvLlv3279D09UqSRpqvwmsCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRjUTAHuP3DXrEiRprjQTAJKk/88AkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktSoXgGQ5LIkjyRZTnJkyPYkuaHb/kCSA+P6JnlDkgeTfCvJ4nSGI0nqa2wAJNkB3AgcBPYDVyXZP9DsILCvuxwGburR9/PATwOf2vwwJEmT6nMEcAmwXFWPVtXTwB3AoYE2h4DbatU9wPlJLhzVt6oeqqpHpjYSSdJE+gTALuDxNcsnunV92vTpO1KSw0mWkiytrKxM0lWSNEKfAMiQddWzTZ++I1XVLVW1WFWLCwsLk3SVJI3QJwBOAHvWLO8GnujZpk9fSWeQ/zmSTusTAPcC+5JclOQc4Erg6ECbo8DV3dlAlwJPVtXJnn0lSTOwc1yDqjqV5DrgbmAHcGtVPZjk2m77zcAx4HJgGXgKuGZUX4Akrwf+CFgA7kpyf1W9ZtoDlCQNNzYAAKrqGKsv8mvX3bzm7wLe3Ldvt/5O4M5JipV05uw9chePXf/aWZehLeQ3gSWpUQaAJDXKAJDmmGfsaCsZAJLUKANA0lzzKGjrGACS1CgDQJIa1VQAeCgpSd/RVABIkr7DAJCkRhkAUgOc/tQwBoAkNcoA0NzYLu9St0ud0jgGgCQ1ygDYAN8Bbi1v3/njfXJ2MgAknRGGyPwxACTNnOEwGwaA5oIvANKZ11wA+EKjvkY9VublcTQvdWh7ai4ApLONIaCNMgC2obPtCT9uPGdqvGfb7bqdeV+cGQbAJvggXd92um0mmeoZtzzp/qVZajoANvLEbP3JPOvxb/X1z3p8w8xjTTo7NB0A2hhfkDZnM7ffen33HrlrqvdL3335WNjeDABtyDxOfWzFC+uszFs9MJ81aXOaDIBxc77b4YE+DzVOo4aNzKlPus+NtJ2H23ecjbxL3w7jWmu71bvdNBkAMPyFZxovHGeqfyu28nZymmNrAnizNejMaTYApqX1B+/p8U96O2zkSGvSL2YNe+e7naalpm1eTrfV/DAA1jHuBaXP+o1c59nwJGzlTJ1JHiOj9rGV9/u83FaaT80HwLx8HrCRszvO1Gms05wam+Xt2fcoYKO397Tm2qcRLJtxJq5rO3/2cjbpFQBJLkvySJLlJEeGbE+SG7rtDyQ5MK5vku9P8vEk/9L9+9zpDGlyZ/pFa1Znq2x2vncjT9qtGuvaF/N5fdHYbNiOur/O5GN2Vh8iz+v9ejYZGwBJdgA3AgeB/cBVSfYPNDsI7Osuh4GbevQ9AnyiqvYBn+iW51afJ9zgE2XwHed6Hzyv90Tv88Rb7zr7thtW56gxjNI3IDayr77virfqg91pHSlu9VlO8/RmZSv2o+lKVY1ukLwceFdVvaZbfjtAVf32mjZ/Anyyqj7QLT8C/Ciwd72+p9tU1ckkF3b9XzSqlsXFxVpaWtrIOH0ATuCx61/r7dWQSe/vx65/LTD8OXV621pr223msTVs3+onyfGqWhxcv7NH313A42uWTwAv69Fm15i+z6+qkwBdCDxvncIPs3pUAfDNLjg24gLg6xvsu5Xmrq78DjCHdXWsazJj6+ru795GtR+3rzXbJ769Jq1zE7btfTnCDw5b2ScAMmTd4GHDem369B2pqm4BbpmkzzBJloYl4KxZ12SsazLWNbl5rW0r6urzIfAJYM+a5d3AEz3bjOr71W7qh+7fr/UvW5K0WX0C4F5gX5KLkpwDXAkcHWhzFLi6OxvoUuDJbnpnVN+jwBu7v98I/PUmxyJJmsDYKaCqOpXkOuBuYAdwa1U9mOTabvvNwDHgcmAZeAq4ZlTfbtfXAx9M8gvAl4A3THVk323T00hbxLomY12Tsa7JzWttU69r7FlAkqSzU/PfBJakVhkAktSoJgJg3E9ZzEKSW5N8LcnnZ13LWkn2JPn7JA8leTDJW2ZdE0CSZyX5pySf7er6jVnXtFaSHUk+k+Sjs67ltCSPJflckvuTbOwblFsgyflJPpTk4e5x9vI5qOlF3e10+vKNJG+ddV0ASX6pe8x/PskHkjxravs+2z8D6H6O4gvAq1k9LfVe4Kqq+ucZ1/VK4JvAbVX14lnWslZ3Su6FVXVfku8DjgM/NQe3V4DzquqbSZ4J/CPwlqq6Z5Z1nZbkl4FF4DlV9bpZ1wOrAQAsVtVcfakpyV8A/1BV7+3ODjy3qv5jxmV9W/ea8WXgZVX1xRnXsovVx/r+qvrvJB8EjlXVn09j/y0cAVwCLFfVo1X1NHAHcGjGNVFVnwL+fdZ1DKqqk1V1X/f3fwIPsfqN7pmqVd/sFp/ZXebi3UuS3cBrgffOupZ5l+Q5wCuB9wFU1dPz9OLfeRXwr7N+8V9jJ/DsJDuBc/nu72FtWAsBsN7PVGiMJHuBlwKfnnEpwLenWe5n9UuDH6+quagL+EPgbcC3ZlzHoAI+luR495Mq8+CHgBXgz7ops/cmOW/WRQ24EvjArIsAqKovA7/H6qnyJ1n9jtXHprX/FgJg0z9H0aIk3wt8GHhrVX1j1vUAVNX/VtXFrH6j/JIkM586S/I64GtVdXzWtQzxiqo6wOqv8b65m3actZ3AAeCmqnop8F/M0S8Bd1NSVwB/NetaALqfyT8EXAS8ADgvyc9Oa/8tBECfn7LQGt0c+4eB26vqI7OuZ1A3ZfBJ4LLZVgLAK4Aruvn2O4AfT/KXsy1pVVU90f37NeBOVqdDZ+0EcGLN0duHWA2EeXEQuK+qvjrrQjo/AfxbVa1U1f8AHwF+ZFo7byEA+vyUhTrdh63vAx6qqj+YdT2nJVlIcn7397NZfWI8PNOigKp6e1Xtrqq9rD62/q6qpvYObaOSnNd9iE83xfKTwMzPOKuqrwCPJzn90++vAmZ6gsGAq5iT6Z/Ol4BLk5zbPTdfxernclPR59dAt7UxP0cxM0k+wOr/mXBBkhPAr1fV+2ZbFbD6jvbngM918+0Av1ZVx2ZXEgAXAn/RnaHxDOCDVTU3p1zOoecDd66+ZrATeH9V/e1sS/q2XwRu796QPUr30zGzluRcVs8WfNOsazmtqj6d5EPAfcAp4DNM8SchzvrTQCVJw7UwBSRJGsIAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY36P9ZXFbRjmtPiAAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(diffComp,bins=np.arange(0,8,0.001),weights=weightsDiff,log = False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
