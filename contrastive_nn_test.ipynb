{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "%run helpers_preproc.ipynb\n",
    "%run alternate_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "origDataDir = 'SHREC11/'\n",
    "newDataDir = 'SHREC11_plus/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate additional data and update label\n",
    "origLabel = np.array(readLbl(600,origDataDir + 'labels.txt'),dtype = int) #first take in the original labels\n",
    "newLabelFile = open(newDataDir + \"labels.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "radiuss = [0.0002,0.0004,0.0008,0.0016]\n",
    "\n",
    "for i in range(600):\n",
    "    #First create the new obj file\n",
    "    origN = origDataDir + 'T' + str(i) + '.obj'\n",
    "    \n",
    "    for j in range(len(radiuss)):\n",
    "        radius = radiuss[j]\n",
    "        newN = newDataDir + 'T' + str(i+600*(j)) + '.obj'\n",
    "    \n",
    "        #newLabelFile.write(\"T\" + str(i) + \".obj\" + \" \" + str(origLabel[i]) + \"\\n\")\n",
    "        if not (write_new_files(origN,newN,radius)):\n",
    "            print('Data generation failed on ' , i)\n",
    "        \n",
    "        newLabelFile.write(\"T\" + str(i + 600*(j)) + \".obj\" + \" \" + str(origLabel[i]) + \"\\n\")\n",
    "\n",
    "    \n",
    "newLabelFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the data, get normalized adjacency (NxN)\n",
    "num_meshes = 600 * 4\n",
    "train_size = 450 * 4\n",
    "test_size = 150 * 4\n",
    "mesh_dir = 'SHREC11_plus/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_swaps = 10000\n",
    "\n",
    "labels = np.array(readLbl(num_meshes,mesh_dir+'labels.txt'))\n",
    "\n",
    "for swap_idx in range(num_swaps):\n",
    "    idxA = random.randint(0,num_meshes-1)\n",
    "    idxB = random.randint(0,num_meshes-1)\n",
    "    \n",
    "    if idxA != idxB:\n",
    "        labelA = labels[idxA]\n",
    "        labelB = labels[idxB]\n",
    "        labels[idxA] = labelB\n",
    "        labels[idxB] = labelA\n",
    "\n",
    "        #change filenames\n",
    "        os.rename(mesh_dir+'T' + str(idxA) + '.obj',mesh_dir+'temp.obj')\n",
    "        os.rename(mesh_dir+'T' + str(idxB) + '.obj',mesh_dir+'T' + str(idxA) + '.obj')\n",
    "        os.rename(mesh_dir+'temp.obj',mesh_dir+'T' + str(idxB) + '.obj')\n",
    "\n",
    "newLabelFile = open(newDataDir + \"labels.txt\", \"w\")\n",
    "for i in range(num_meshes):\n",
    "        newLabelFile.write(\"T\" + str(i) + \".obj\" + \" \" + str(labels[i]) + \"\\n\")\n",
    "\n",
    "\n",
    "newLabelFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_np = np.array(readLbl(num_meshes,mesh_dir+'labels.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_sigs_list = []\n",
    "normed_adjMats_list = []\n",
    "\n",
    "for i in range(num_meshes):\n",
    "    fName = 'T' + str(i) + '.obj'\n",
    "    adj_noscale_read = obj_2_adj_noscale(mesh_dir + fName)\n",
    "    adj_scaled_read = obj_2_adj(mesh_dir + fName)\n",
    "\n",
    "    if (adj_scaled_read.shape[0] < 252):\n",
    "        adj_noscale = np.empty([252,252])\n",
    "        adj_scaled = np.empty([252,252])\n",
    "    else:\n",
    "        adj_noscale = adj_noscale_read\n",
    "        adj_scaled = adj_scaled_read\n",
    "\n",
    "    adj_normalized = adj_noscale / np.reshape(np.sum(adj_noscale,axis = 1),[adj_noscale.shape[0],1])\n",
    "\n",
    "    #node level signal extraction\n",
    "    node_degs = np.sum(adj_noscale,axis = 0)\n",
    "    node_neigh_max = np.max(adj_scaled,axis = 0)\n",
    "    #node_neigh_min = np.min(adj_scaled,axis = 0)\n",
    "    node_neigh_sum = np.sum(adj_scaled,axis = 0)\n",
    "    node_neigh_mean = np.sum(adj_scaled,axis = 0) / node_degs\n",
    "\n",
    "    node_sig = np.stack([node_degs,node_neigh_max,node_neigh_sum,node_neigh_mean],axis = 1)\n",
    "    \n",
    "    node_sigs_list.append(node_sig)\n",
    "    normed_adjMats_list.append(adj_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_adjMats = np.nan_to_num(np.stack(normed_adjMats_list))\n",
    "node_sigs = np.nan_to_num(np.stack(node_sigs_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_mean = np.reshape(np.mean(node_sigs,axis = 1),[num_meshes,1,5])\n",
    "#node_sigs = node_sigs - feature_mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[8.        , 0.21602031, 0.93197489, 0.11649686],\n",
       "        [6.        , 0.19737095, 1.00369017, 0.1672817 ],\n",
       "        [5.        , 0.16103456, 0.44536618, 0.08907324],\n",
       "        ...,\n",
       "        [6.        , 0.12915873, 0.50090544, 0.08348424],\n",
       "        [7.        , 0.19328788, 1.12109104, 0.16015586],\n",
       "        [7.        , 0.2305088 , 1.24528015, 0.17789716]],\n",
       "\n",
       "       [[5.        , 0.06210198, 0.23493933, 0.04698787],\n",
       "        [5.        , 0.11466726, 0.3035657 , 0.06071314],\n",
       "        [3.        , 0.0375809 , 0.10481666, 0.03493889],\n",
       "        ...,\n",
       "        [8.        , 0.10067814, 0.48646736, 0.06080842],\n",
       "        [6.        , 0.12029223, 0.56660735, 0.09443456],\n",
       "        [6.        , 0.09960374, 0.38744729, 0.06457455]],\n",
       "\n",
       "       [[5.        , 0.11337228, 0.38616764, 0.07723353],\n",
       "        [5.        , 0.08760865, 0.37376395, 0.07475279],\n",
       "        [5.        , 0.10911618, 0.38778972, 0.07755794],\n",
       "        ...,\n",
       "        [6.        , 0.10598813, 0.50822867, 0.08470478],\n",
       "        [5.        , 0.09377536, 0.43900484, 0.08780097],\n",
       "        [5.        , 0.09377536, 0.42728739, 0.08545748]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[4.        , 0.08647052, 0.27888368, 0.06972092],\n",
       "        [4.        , 0.09733106, 0.29100181, 0.07275045],\n",
       "        [5.        , 0.08842707, 0.37284134, 0.07456827],\n",
       "        ...,\n",
       "        [5.        , 0.10545783, 0.41868969, 0.08373794],\n",
       "        [6.        , 0.12563312, 0.60621417, 0.10103569],\n",
       "        [5.        , 0.13280854, 0.44752129, 0.08950426]],\n",
       "\n",
       "       [[4.        , 0.1321003 , 0.29319959, 0.0732999 ],\n",
       "        [5.        , 0.25872976, 0.81499431, 0.16299886],\n",
       "        [5.        , 0.30192199, 0.58708536, 0.11741707],\n",
       "        ...,\n",
       "        [5.        , 0.13484872, 0.60524254, 0.12104851],\n",
       "        [6.        , 0.17189136, 0.84171489, 0.14028582],\n",
       "        [6.        , 0.17189136, 0.74422956, 0.12403826]],\n",
       "\n",
       "       [[5.        , 0.19721463, 0.62129022, 0.12425804],\n",
       "        [5.        , 0.16501718, 0.56357191, 0.11271438],\n",
       "        [5.        , 0.19721463, 0.57654014, 0.11530803],\n",
       "        ...,\n",
       "        [6.        , 0.23209947, 0.9322325 , 0.15537208],\n",
       "        [8.        , 0.14190079, 0.7328183 , 0.09160229],\n",
       "        [5.        , 0.14315604, 0.44118666, 0.08823733]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_sigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_adjMats_tr = normed_adjMats[:train_size,:,:]\n",
    "node_sigs_tr = node_sigs[:train_size,:,:]\n",
    "label_np_tr = label_np[:train_size]\n",
    "\n",
    "normed_adjMats_ts = normed_adjMats[train_size:,:,:]\n",
    "node_sigs_ts = node_sigs[train_size:,:,:]\n",
    "label_np_ts = label_np[train_size:]\n",
    "\n",
    "label_mat_tr = torch.tensor(np.where(igl.all_pairs_distances(label_np_tr,label_np_tr,False) > 0.5,0,1)).float()\n",
    "label_mat_ts = torch.tensor(np.where(igl.all_pairs_distances(label_np_ts,label_np_ts,False) > 0.5,0,1)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_adjMats_tr = torch.tensor(normed_adjMats_tr).float()\n",
    "node_sigs_tr = torch.tensor(node_sigs_tr).float()\n",
    "label_tr = torch.tensor(label_np_tr).float()\n",
    "\n",
    "normed_adjMats_ts = torch.tensor(normed_adjMats_ts).float()\n",
    "node_sigs_ts = torch.tensor(node_sigs_ts).float()\n",
    "label_ts = torch.tensor(label_np_ts).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_mat_train = form_label_matrix(label_mat, train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normed_adjMats_list_train, node_sigs_list_train = prep_data(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normed_adjMats_train = np.stack(normed_adjMats_list_train)\n",
    "#node_sigs_train = np.stack(node_sigs_list_train)\n",
    "\n",
    "#normed_adjMats_train = torch.tensor(normed_adjMats_train,requires_grad=False).float()\n",
    "#node_sigs_train = torch.tensor(node_sigs_train,requires_grad=False).float()\n",
    "\n",
    "#normed_adjMats_train = torch.nan_to_num(normed_adjMats_train,0,0,0)\n",
    "#node_sigs_train = torch.nan_to_num(node_sigs_train,0,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1800, 252, 4])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_sigs_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1800, 252, 5])\n",
      "torch.Size([1800, 5])\n"
     ]
    }
   ],
   "source": [
    "%run NNs.ipynb\n",
    "g = GCN(4,[5,5,5,5])\n",
    "n = MLP(252*5,[5,5,5,5])\n",
    "output1 = g.forward(normed_adjMats_tr,node_sigs_tr)\n",
    "output2 = n.forward(output1)\n",
    "print(output1.shape)\n",
    "print(output2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lossF(features,lbl_mat):\n",
    "    disMat = torch.cdist(features,features)\n",
    "    sameType = disMat * (lbl_mat-torch.diag(torch.ones(lbl_mat.shape[0])))\n",
    "    diffType = disMat * (1-lbl_mat)\n",
    "\n",
    "    diffTypeScaled = diffType * torch.exp(-diffType)\n",
    "    diffTypeScaledMean = torch.sum(diffTypeScaled)/ torch.count_nonzero(diffTypeScaled)\n",
    "    diffTypeMean = torch.sum(diffType)/ torch.count_nonzero(diffType)\n",
    "\n",
    "    sameTypeMean = torch.sum(sameType) / torch.count_nonzero(sameType)\n",
    "    sameTypeStd = torch.sum((sameType - sameTypeMean)**2) / torch.count_nonzero(sameType)\n",
    "\n",
    "    toRet = sameTypeMean-diffTypeMean + 0.01 * torch.sqrt(sameTypeStd)\n",
    "    #toRet = -(sameTypeMean-diffTypeMean)**2 #+ 0.5*(sameTypeMean + 0.01 * torch.sqrt(sameTypeStd))\n",
    "    #toRet = -(sameTypeMean-diffTypeMean)**2 - diffTypeScaledMean + 0.5 * torch.sqrt(sameTypeStd)\n",
    "    \n",
    "    print(sameTypeMean.detach().numpy(),diffTypeMean.detach().numpy(),torch.sqrt(sameTypeStd).detach().numpy(), toRet.detach().numpy())\n",
    "    return  toRet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00036976344 0.000403406 0.0021424582 -1.2217992e-05\n",
      "0.0003708744 0.0004084153 0.002146539 -1.6075524e-05\n",
      "0.0003767979 0.00041593498 0.0021936316 -1.720077e-05\n",
      "0.0003819111 0.0004260396 0.00221118 -2.2016708e-05\n",
      "0.00038851314 0.0004371264 0.0022480912 -2.6132351e-05\n",
      "0.00039733248 0.00045090355 0.0022975104 -3.059597e-05\n",
      "0.00040804257 0.00046751343 0.0023632138 -3.583872e-05\n",
      "0.0004203311 0.00048621843 0.0024343808 -4.154351e-05\n",
      "0.00043417065 0.00050720415 0.0025129593 -4.7903908e-05\n",
      "0.00044995395 0.00053225714 0.0025990815 -5.6312376e-05\n",
      "0.0004688024 0.00055932585 0.0027133534 -6.338992e-05\n",
      "0.000487974 0.0005900347 0.0028153793 -7.390691e-05\n",
      "0.00050966715 0.00062381185 0.002939007 -8.475463e-05\n",
      "0.00053518714 0.0006607052 0.0030872521 -9.464555e-05\n",
      "0.0005617541 0.0007011435 0.0032334395 -0.00010705502\n",
      "0.0005901331 0.00074442575 0.0033931974 -0.0001203607\n",
      "0.00062063156 0.00079173024 0.0035620604 -0.00013547808\n",
      "0.00065482536 0.0008428088 0.003754455 -0.0001504389\n",
      "0.0006911995 0.0008972579 0.0039553572 -0.0001665048\n",
      "0.00073273364 0.0009566298 0.00419991 -0.00018189708\n",
      "0.00077569525 0.0010200276 0.0044470117 -0.00019986219\n",
      "0.00081886165 0.0010870389 0.0046801413 -0.00022137584\n",
      "0.00086630596 0.0011587908 0.0049465518 -0.00024301931\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t4/vdzymfw562g8ml8ks_2b5l5r0000gn/T/ipykernel_48997/2036407944.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormed_adjMats_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnode_sigs_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_mat_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TSP/lib/python3.10/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
      "\u001b[0;32m~/.conda/envs/TSP/lib/python3.10/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(g.weights + n.weights,lr = 0.0001)\n",
    "for i in range(100000):\n",
    "    optimizer.zero_grad()\n",
    "    output = n.forward(g.forward(normed_adjMats_tr,node_sigs_tr))\n",
    "    loss = lossF(output,label_mat_tr)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = n.forward(g.forward(normed_adjMats_tr,node_sigs_tr))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disMat = torch.cdist(output,output).flatten()\n",
    "mask_same = torch.tensor(label_mat_tr - torch.diag(torch.ones(train_size)),dtype=bool).flatten()\n",
    "mask_diff = torch.tensor(1 - label_mat_tr,dtype=bool).flatten()\n",
    "sameComp = disMat[mask_same].detach().numpy()\n",
    "diffComp = disMat[mask_diff].detach().numpy()\n",
    "weightsSame = np.ones_like(sameComp) / len(sameComp)\n",
    "weightsDiff = np.ones_like(diffComp) / len(diffComp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(sameComp),np.mean(diffComp))\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "ax1.hist(sameComp,bins=np.arange(0,np.max(sameComp),0.01),weights=weightsSame,log = False)\n",
    "ax1.set_title('Same Comp')\n",
    "ax2.hist(diffComp,bins=np.arange(0,np.max(diffComp),0.01),weights=weightsDiff,log = False)\n",
    "ax2.set_title('Diff Comp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = n.forward(g.forward(normed_adjMats_ts,node_sigs_ts))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disMat = torch.cdist(output,output).flatten()\n",
    "mask_same = torch.tensor(label_mat_ts - torch.diag(torch.ones(test_size)),dtype=bool).flatten()\n",
    "mask_diff = torch.tensor(1 - label_mat_ts,dtype=bool).flatten()\n",
    "sameComp = disMat[mask_same].detach().numpy()\n",
    "diffComp = disMat[mask_diff].detach().numpy()\n",
    "weightsSame = np.ones_like(sameComp) / len(sameComp)\n",
    "weightsDiff = np.ones_like(diffComp) / len(diffComp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(sameComp),np.mean(diffComp))\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "ax1.hist(sameComp,bins=np.arange(0,np.max(sameComp),0.01),weights=weightsSame,log = False)\n",
    "ax1.set_title('Same Comp')\n",
    "ax2.hist(diffComp,bins=np.arange(0,np.max(diffComp),0.01),weights=weightsDiff,log = False)\n",
    "ax2.set_title('Diff Comp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from matplotlib.pyplot import figure\n",
    "#figure(figsize=(10, 8), dpi=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BigDiffIndices = np.where(disMat.detach().numpy().flatten() > 0.75)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(BigDiffIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vizMat = np.zeros([30,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating 30 categories into less categories\n",
    "\n",
    "# for index in BigDiffIndices:\n",
    "#     row, col = divmod(index,test_size)\n",
    "#     cat1, cat2 = int(label_np_ts[row]),int(label_np_ts[col])\n",
    "#     if cat1 == cat2:\n",
    "#         print(cat1, cat2)\n",
    "#     vizMat[cat1,cat2] += 1\n",
    "#     vizMat[cat2,cat1] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vizMat = vizMat / np.max(vizMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(vizMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = output.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "newX = pca.fit_transform(features)\n",
    "np.sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_show = [0,1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPlot = []\n",
    "for i in range(len(indices_to_show)):\n",
    "    a = newX[np.where(label_np_ts == indices_to_show[i]),:]\n",
    "    a = np.reshape(a,[a.shape[1],a.shape[2]])\n",
    "    categories = np.ones([a.shape[0],1]) * indices_to_show[i]\n",
    "    b = np.concatenate((a,categories),axis = 1)\n",
    "    dataPlot.append(b)\n",
    "    \n",
    "dataPlot = np.concatenate(dataPlot,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPlot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.scatter(dataPlot[:,0], dataPlot[:,1], dataPlot[:,2], label=dataPlot[:,3], c = dataPlot[:,3], cmap = 'viridis')\n",
    "#ax.axis('off')\n",
    "ax.axes.xaxis.set_ticklabels([])\n",
    "ax.axes.yaxis.set_ticklabels([])\n",
    "ax.axes.zaxis.set_ticklabels([])\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.set_title('[0,1,2,3,4]')\n",
    "\n",
    "#plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
