{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear, ReLU\n",
    "import torch.optim as optim\n",
    "# from torch_geometric.nn import Sequential, GCNConv\n",
    "import torch\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "#from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#read in the data, get normalized adjacency (NxN)\n",
    "%run helpers_preproc.ipynb\n",
    "mesh_dir = 'SHREC11/'\n",
    "\n",
    "label_np = np.array(readLbl(mesh_dir+'labels.txt'))\n",
    "label_mat_np = np.where(igl.all_pairs_distances(label_np,label_np,False) > 0.5,0,1)\n",
    "label_mat = torch.tensor(label_mat_np,requires_grad=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "adjMats = []\n",
    "\n",
    "for i in range(600):\n",
    "    fName = 'T' + str(i) + '.obj'\n",
    "    adj_noscale = obj_2_adj_noscale(mesh_dir + fName)\n",
    "    adj = obj_2_adj(mesh_dir + fName)\n",
    "    adj_exp = np.exp(-adj) * adj_noscale\n",
    "    adj = adj_exp\n",
    "    adj = adj_noscale\n",
    "    adj = adj / np.reshape(np.sum(adj,axis = 1),[adj.shape[0],1])\n",
    "    if (adj.shape[0] <252):\n",
    "        tempAdj = np.empty([252,252])\n",
    "        tempAdj[:adj.shape[0],:adj.shape[0]] = adj\n",
    "        adjMats.append(tempAdj)\n",
    "    else:\n",
    "        adjMats.append(adj)\n",
    "\n",
    "adjMats = np.array(adjMats)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.        , 0.2       , 0.2       , ..., 0.        , 0.        ,\n        0.        ],\n       [0.14285714, 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.16666667, 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       ...,\n       [0.        , 0.        , 0.        , ..., 0.        , 0.14285714,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.16666667, 0.        ,\n        0.16666667],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.14285714,\n        0.        ]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjMats[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, widths):\n",
    "        super().__init__()\n",
    "        self.f = 1\n",
    "        self.widths = widths\n",
    "        self.weights = []\n",
    "        self.weights.append(torch.autograd.Variable(torch.rand(size = [self.f,widths[0]])-0.5,requires_grad = True))\n",
    "\n",
    "        for i in range(1,len(self.widths)):\n",
    "            c = self.widths[i]\n",
    "            self.weights.append(torch.autograd.Variable(torch.rand((self.widths[i-1],self.widths[i]))-0.5,requires_grad = True))\n",
    "\n",
    "    def forward(self, adjM,node_sigM):\n",
    "        curr_output = torch.tanh(torch.matmul(torch.matmul(adjM,node_sigM),self.weights[0])) #nxc\n",
    "        for i in range(1,len(self.widths)):\n",
    "            curr_output = torch.tanh(torch.matmul(torch.matmul(adjM,curr_output),self.weights[i])) #nxc\n",
    "\n",
    "        return curr_output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class NNN(torch.nn.Module):\n",
    "    def __init__(self, widths):\n",
    "        super().__init__()\n",
    "        self.input_shape = 25200\n",
    "        self.widths = widths\n",
    "        self.weights = []\n",
    "        self.weights.append(torch.autograd.Variable(torch.rand(size = [self.input_shape,widths[0]])-0.5,requires_grad = True))\n",
    "\n",
    "        for i in range(1,len(self.widths)):\n",
    "            c = self.widths[i]\n",
    "            self.weights.append(torch.autograd.Variable(torch.rand((self.widths[i-1],self.widths[i]))-0.5,requires_grad = True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x,1,2)\n",
    "        torch.matmul(x,self.weights[0])\n",
    "        curr_output = torch.tanh(torch.matmul(x,self.weights[0])) #nxc\n",
    "        for i in range(1,len(self.widths)):\n",
    "            curr_output = torch.tanh(torch.matmul(curr_output,self.weights[i])) #nxc\n",
    "\n",
    "        return curr_output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj = torch.tensor(adjMats, requires_grad= False).float()\n",
    "node_sig = torch.tensor(np.ones([600,252,1]),requires_grad= False).float()\n",
    "\n",
    "check = torch.sum(torch.isnan(adj))\n",
    "check"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9974,  1.0000, -1.0000,  ...,  1.0000, -1.0000, -1.0000],\n",
      "        [ 0.9974,  1.0000, -1.0000,  ...,  1.0000, -1.0000, -1.0000],\n",
      "        [ 0.9974,  1.0000, -1.0000,  ...,  1.0000, -1.0000, -1.0000],\n",
      "        ...,\n",
      "        [ 0.9974,  1.0000, -1.0000,  ...,  1.0000, -1.0000, -1.0000],\n",
      "        [ 0.9974,  1.0000, -1.0000,  ...,  1.0000, -1.0000, -1.0000],\n",
      "        [ 0.9974,  1.0000, -1.0000,  ...,  1.0000, -1.0000, -1.0000]],\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "g = GCN([100])\n",
    "n = NNN([100])\n",
    "output1 = g.forward(adj,node_sig)\n",
    "output2 = n.forward(output1)\n",
    "print(output2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def lossF(features):\n",
    "    print(features)\n",
    "    pdist = torch.nn.PairwiseDistance(p=2)\n",
    "    pairDist = pdist(features,features)\n",
    "    print(pairDist)\n",
    "    return  -torch.mean(pairDist)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Adam.__init__() got multiple values for argument 'lr'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m \u001B[43moptim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAdam\u001B[49m\u001B[43m(\u001B[49m\u001B[43mg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweights\u001B[49m\u001B[43m,\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.001\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m10\u001B[39m):\n\u001B[1;32m      4\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "\u001B[0;31mTypeError\u001B[0m: Adam.__init__() got multiple values for argument 'lr'"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(g.weights + n.weights,lr = 0.001)\n",
    "\n",
    "for i in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    output = n.forward(g.forward(adj,node_sig))\n",
    "    loss = lossF(output)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(g.weights[0].grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mat = np.empty([30,30])\n",
    "plt.matshow(mat)\n",
    "self_mean = np.mean(compSelf)\n",
    "self_std = np.std(compSelf)\n",
    "others_mean = np.mean(compOthers)\n",
    "others_std = np.std(compOthers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "c1 = 10\n",
    "c2 = 20\n",
    "c3 = 30\n",
    "\n",
    "weights_1 = torch.rand(size = [1,c1], requires_grad= True)\n",
    "weights_2 = torch.rand([c1,c2],requires_grad= True)\n",
    "weights_3 = torch.rand([c2,c3], requires_grad= False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [],
   "source": [
    "def GCN(adjM,node_sigM):\n",
    "    layer_1_output = torch.matmul(torch.matmul(adjM,node_sigM),weights_1) #nxc\n",
    "    #layer 2\n",
    "    layer_2_output = torch.matmul(torch.matmul(adj,layer_1_output),weights_2) #nxc\n",
    "    #layer 2\n",
    "    layer_3_output = torch.matmul(torch.matmul(adj,layer_2_output),weights_3) #nxc\n",
    "    return layer_3_output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(439.9774, grad_fn=<MeanBackward0>)\n",
      "tensor(421.3802, grad_fn=<MeanBackward0>)\n",
      "tensor(403.1801, grad_fn=<MeanBackward0>)\n",
      "tensor(385.3835, grad_fn=<MeanBackward0>)\n",
      "tensor(367.9957, grad_fn=<MeanBackward0>)\n",
      "tensor(351.0208, grad_fn=<MeanBackward0>)\n",
      "tensor(334.4612, grad_fn=<MeanBackward0>)\n",
      "tensor(318.3172, grad_fn=<MeanBackward0>)\n",
      "tensor(302.5864, grad_fn=<MeanBackward0>)\n",
      "tensor(287.2630, grad_fn=<MeanBackward0>)\n",
      "tensor(272.3385, grad_fn=<MeanBackward0>)\n",
      "tensor(257.8025, grad_fn=<MeanBackward0>)\n",
      "tensor(243.6440, grad_fn=<MeanBackward0>)\n",
      "tensor(229.8531, grad_fn=<MeanBackward0>)\n",
      "tensor(216.4211, grad_fn=<MeanBackward0>)\n",
      "tensor(203.3407, grad_fn=<MeanBackward0>)\n",
      "tensor(190.6051, grad_fn=<MeanBackward0>)\n",
      "tensor(178.2077, grad_fn=<MeanBackward0>)\n",
      "tensor(166.1418, grad_fn=<MeanBackward0>)\n",
      "tensor(154.4001, grad_fn=<MeanBackward0>)\n",
      "tensor(142.9748, grad_fn=<MeanBackward0>)\n",
      "tensor(131.8571, grad_fn=<MeanBackward0>)\n",
      "tensor(121.0378, grad_fn=<MeanBackward0>)\n",
      "tensor(110.5065, grad_fn=<MeanBackward0>)\n",
      "tensor(100.2522, grad_fn=<MeanBackward0>)\n",
      "tensor(90.2631, grad_fn=<MeanBackward0>)\n",
      "tensor(80.5265, grad_fn=<MeanBackward0>)\n",
      "tensor(71.0291, grad_fn=<MeanBackward0>)\n",
      "tensor(61.7568, grad_fn=<MeanBackward0>)\n",
      "tensor(52.6948, grad_fn=<MeanBackward0>)\n",
      "tensor(43.8281, grad_fn=<MeanBackward0>)\n",
      "tensor(35.1407, grad_fn=<MeanBackward0>)\n",
      "tensor(26.6167, grad_fn=<MeanBackward0>)\n",
      "tensor(18.2395, grad_fn=<MeanBackward0>)\n",
      "tensor(9.9925, grad_fn=<MeanBackward0>)\n",
      "tensor(1.8588, grad_fn=<MeanBackward0>)\n",
      "tensor(-6.1785, grad_fn=<MeanBackward0>)\n",
      "tensor(-14.1364, grad_fn=<MeanBackward0>)\n",
      "tensor(-22.0319, grad_fn=<MeanBackward0>)\n",
      "tensor(-29.8819, grad_fn=<MeanBackward0>)\n",
      "tensor(-37.7034, grad_fn=<MeanBackward0>)\n",
      "tensor(-45.5130, grad_fn=<MeanBackward0>)\n",
      "tensor(-53.3276, grad_fn=<MeanBackward0>)\n",
      "tensor(-61.1637, grad_fn=<MeanBackward0>)\n",
      "tensor(-69.0377, grad_fn=<MeanBackward0>)\n",
      "tensor(-76.9660, grad_fn=<MeanBackward0>)\n",
      "tensor(-84.9648, grad_fn=<MeanBackward0>)\n",
      "tensor(-93.0503, grad_fn=<MeanBackward0>)\n",
      "tensor(-101.2383, grad_fn=<MeanBackward0>)\n",
      "tensor(-109.5449, grad_fn=<MeanBackward0>)\n",
      "tensor(-117.9855, grad_fn=<MeanBackward0>)\n",
      "tensor(-126.5759, grad_fn=<MeanBackward0>)\n",
      "tensor(-135.3314, grad_fn=<MeanBackward0>)\n",
      "tensor(-144.2675, grad_fn=<MeanBackward0>)\n",
      "tensor(-153.3993, grad_fn=<MeanBackward0>)\n",
      "tensor(-162.7419, grad_fn=<MeanBackward0>)\n",
      "tensor(-172.3102, grad_fn=<MeanBackward0>)\n",
      "tensor(-182.1190, grad_fn=<MeanBackward0>)\n",
      "tensor(-192.1831, grad_fn=<MeanBackward0>)\n",
      "tensor(-202.5170, grad_fn=<MeanBackward0>)\n",
      "tensor(-213.1351, grad_fn=<MeanBackward0>)\n",
      "tensor(-224.0519, grad_fn=<MeanBackward0>)\n",
      "tensor(-235.2814, grad_fn=<MeanBackward0>)\n",
      "tensor(-246.8379, grad_fn=<MeanBackward0>)\n",
      "tensor(-258.7353, grad_fn=<MeanBackward0>)\n",
      "tensor(-270.9875, grad_fn=<MeanBackward0>)\n",
      "tensor(-283.6082, grad_fn=<MeanBackward0>)\n",
      "tensor(-296.6109, grad_fn=<MeanBackward0>)\n",
      "tensor(-310.0094, grad_fn=<MeanBackward0>)\n",
      "tensor(-323.8168, grad_fn=<MeanBackward0>)\n",
      "tensor(-338.0464, grad_fn=<MeanBackward0>)\n",
      "tensor(-352.7115, grad_fn=<MeanBackward0>)\n",
      "tensor(-367.8248, grad_fn=<MeanBackward0>)\n",
      "tensor(-383.3992, grad_fn=<MeanBackward0>)\n",
      "tensor(-399.4475, grad_fn=<MeanBackward0>)\n",
      "tensor(-415.9821, grad_fn=<MeanBackward0>)\n",
      "tensor(-433.0157, grad_fn=<MeanBackward0>)\n",
      "tensor(-450.5603, grad_fn=<MeanBackward0>)\n",
      "tensor(-468.6283, grad_fn=<MeanBackward0>)\n",
      "tensor(-487.2315, grad_fn=<MeanBackward0>)\n",
      "tensor(-506.3821, grad_fn=<MeanBackward0>)\n",
      "tensor(-526.0918, grad_fn=<MeanBackward0>)\n",
      "tensor(-546.3723, grad_fn=<MeanBackward0>)\n",
      "tensor(-567.2352, grad_fn=<MeanBackward0>)\n",
      "tensor(-588.6923, grad_fn=<MeanBackward0>)\n",
      "tensor(-610.7548, grad_fn=<MeanBackward0>)\n",
      "tensor(-633.4343, grad_fn=<MeanBackward0>)\n",
      "tensor(-656.7421, grad_fn=<MeanBackward0>)\n",
      "tensor(-680.6898, grad_fn=<MeanBackward0>)\n",
      "tensor(-705.2886, grad_fn=<MeanBackward0>)\n",
      "tensor(-730.5502, grad_fn=<MeanBackward0>)\n",
      "tensor(-756.4858, grad_fn=<MeanBackward0>)\n",
      "tensor(-783.1068, grad_fn=<MeanBackward0>)\n",
      "tensor(-810.4249, grad_fn=<MeanBackward0>)\n",
      "tensor(-838.4518, grad_fn=<MeanBackward0>)\n",
      "tensor(-867.1990, grad_fn=<MeanBackward0>)\n",
      "tensor(-896.6787, grad_fn=<MeanBackward0>)\n",
      "tensor(-926.9025, grad_fn=<MeanBackward0>)\n",
      "tensor(-957.8828, grad_fn=<MeanBackward0>)\n",
      "tensor(-989.6315, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = optim.Adam([weights_1,weights_2,weights_3],lr = 0.01)\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = GCN(adj,node_sig)\n",
    "    #output = torch.mean(weights_1)\n",
    "    loss = torch.mean(output)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1., 1., 1.])"
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([450,500,300])\n",
    "torch.tanh(a)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "# adj_tensor = torch.tensor(adjMats)\n",
    "# edge_indx = adj_tensor.nonzero().t().contiguous()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [],
   "source": [
    "# # print(edge_indx)\n",
    "# # print(edge_indx.shape)\n",
    "# print(torch.max(edge_indx))\n",
    "# print(edge_indx.size())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [],
   "source": [
    "# data = torch_geometric.data.Data(x = torch.zeros(252,1), edge_index=edge_indx)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [],
   "source": [
    "# #hyper parameters\n",
    "# in_channels = 252\n",
    "# out_channels = 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "# #set up gnn\n",
    "# model = Sequential('x, edge_index', [\n",
    "#     (GCNConv(in_channels, 64), 'x, edge_index -> x'),\n",
    "#     ReLU(inplace=True),\n",
    "#     (GCNConv(64, 64), 'x, edge_index -> x'),\n",
    "#     ReLU(inplace=True),\n",
    "#     Linear(64, out_channels),\n",
    "# ])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [],
   "source": [
    "# data = torch.tensor(adjMats).float()\n",
    "#\n",
    "# model(data,edge_indx)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#run gnn, get (NxC) matrix\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#compute contrastive loss\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#update weights (FxC)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
