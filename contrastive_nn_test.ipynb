{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#read in the data, get normalized adjacency (NxN)\n",
    "%run helpers_preproc.ipynb\n",
    "mesh_dir = 'SHREC11/'\n",
    "\n",
    "label_np = np.array(readLbl(mesh_dir+'labels.txt'))\n",
    "label_mat_np = np.where(igl.all_pairs_distances(label_np,label_np,False) > 0.5,0,1)\n",
    "label_mat = torch.tensor(label_mat_np,requires_grad=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "adjMats = []\n",
    "\n",
    "for i in range(600):\n",
    "    fName = 'T' + str(i) + '.obj'\n",
    "    adj_noscale = obj_2_adj_noscale(mesh_dir + fName)\n",
    "    adj = obj_2_adj(mesh_dir + fName)\n",
    "    adj_exp = np.exp(-adj) * adj_noscale\n",
    "    adj = adj_exp\n",
    "    adj = adj_noscale\n",
    "    adj = adj / np.reshape(np.sum(adj,axis = 1),[adj.shape[0],1])\n",
    "    if (adj.shape[0] <252):\n",
    "        tempAdj = np.empty([252,252])\n",
    "        tempAdj[:adj.shape[0],:adj.shape[0]] = adj\n",
    "        adjMats.append(tempAdj)\n",
    "    else:\n",
    "        adjMats.append(adj)\n",
    "\n",
    "adjMats = np.array(adjMats)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.        , 0.2       , 0.2       , ..., 0.        , 0.        ,\n        0.        ],\n       [0.14285714, 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.16666667, 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       ...,\n       [0.        , 0.        , 0.        , ..., 0.        , 0.14285714,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.16666667, 0.        ,\n        0.16666667],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.14285714,\n        0.        ]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjMats[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, widths):\n",
    "        super().__init__()\n",
    "        self.f = 1\n",
    "        self.widths = widths\n",
    "        self.weights = []\n",
    "        self.weights.append(torch.autograd.Variable(torch.rand(size = [self.f,widths[0]])-0.5,requires_grad = True))\n",
    "\n",
    "        for i in range(1,len(self.widths)):\n",
    "            c = self.widths[i]\n",
    "            self.weights.append(torch.autograd.Variable(torch.rand((self.widths[i-1],self.widths[i]))-0.5,requires_grad = True))\n",
    "\n",
    "    def forward(self, adjM,node_sigM):\n",
    "        curr_output = torch.tanh(torch.matmul(torch.matmul(adjM,node_sigM),self.weights[0])) #nxc\n",
    "        for i in range(1,len(self.widths)):\n",
    "            curr_output = torch.tanh(torch.matmul(torch.matmul(adjM,curr_output),self.weights[i])) #nxc\n",
    "\n",
    "        return curr_output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class NNN(torch.nn.Module):\n",
    "    def __init__(self, widths):\n",
    "        super().__init__()\n",
    "        self.input_shape = 2520\n",
    "        self.widths = widths\n",
    "        self.weights = []\n",
    "        self.weights.append(torch.autograd.Variable(torch.rand(size = [self.input_shape,widths[0]])-0.5,requires_grad = True))\n",
    "\n",
    "        for i in range(1,len(self.widths)):\n",
    "            c = self.widths[i]\n",
    "            self.weights.append(torch.autograd.Variable(torch.rand((self.widths[i-1],self.widths[i]))-0.5,requires_grad = True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x,1,2)\n",
    "        torch.matmul(x,self.weights[0])\n",
    "        curr_output = torch.tanh(torch.matmul(x,self.weights[0])) #nxc\n",
    "        for i in range(1,len(self.widths)):\n",
    "            curr_output = torch.tanh(torch.matmul(curr_output,self.weights[i])) #nxc\n",
    "\n",
    "        return curr_output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj = torch.tensor(adjMats, requires_grad= False).float()\n",
    "node_sig = torch.tensor(np.ones([600,252,1]),requires_grad= False).float()\n",
    "\n",
    "check = torch.sum(torch.isnan(adj))\n",
    "check"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000, -0.9816, -1.0000,  ..., -0.0126, -0.6676, -0.9973],\n",
      "        [ 1.0000, -0.9816, -1.0000,  ..., -0.0126, -0.6676, -0.9973],\n",
      "        [ 1.0000, -0.9816, -1.0000,  ..., -0.0126, -0.6676, -0.9973],\n",
      "        ...,\n",
      "        [ 1.0000, -0.9816, -1.0000,  ..., -0.0126, -0.6676, -0.9973],\n",
      "        [ 1.0000, -0.9816, -1.0000,  ..., -0.0126, -0.6676, -0.9973],\n",
      "        [ 1.0000, -0.9816, -1.0000,  ..., -0.0126, -0.6676, -0.9973]],\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "g = GCN([10])\n",
    "n = NNN([10])\n",
    "output1 = g.forward(adj,node_sig)\n",
    "output2 = n.forward(output1)\n",
    "print(output2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def lossF(features):\n",
    "    disMat = torch.cdist(features,features)\n",
    "    sameType = disMat * label_mat\n",
    "    diffType = disMat * (1-label_mat)\n",
    "    sameTypeMean = torch.sum(sameType) / 12000\n",
    "    diffTypeMean = torch.sum(diffType) /348000\n",
    "    #print(disMat)\n",
    "    return  sameTypeMean - diffTypeMean"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0208, grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[58], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m      5\u001B[0m output \u001B[38;5;241m=\u001B[39m n\u001B[38;5;241m.\u001B[39mforward(g\u001B[38;5;241m.\u001B[39mforward(adj,node_sig))\n\u001B[0;32m----> 6\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mlossF\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m      8\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "Cell \u001B[0;32mIn[47], line 2\u001B[0m, in \u001B[0;36mlossF\u001B[0;34m(features)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlossF\u001B[39m(features):\n\u001B[0;32m----> 2\u001B[0m     disMat \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcdist\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m     sameType \u001B[38;5;241m=\u001B[39m disMat \u001B[38;5;241m*\u001B[39m label_mat\n\u001B[1;32m      4\u001B[0m     diffType \u001B[38;5;241m=\u001B[39m disMat \u001B[38;5;241m*\u001B[39m (\u001B[38;5;241m1\u001B[39m\u001B[38;5;241m-\u001B[39mlabel_mat)\n",
      "File \u001B[0;32m~/.conda/envs/TSM/lib/python3.10/site-packages/torch/functional.py:1214\u001B[0m, in \u001B[0;36mcdist\u001B[0;34m(x1, x2, p, compute_mode)\u001B[0m\n\u001B[1;32m   1211\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m   1212\u001B[0m         cdist, (x1, x2), x1, x2, p\u001B[38;5;241m=\u001B[39mp, compute_mode\u001B[38;5;241m=\u001B[39mcompute_mode)\n\u001B[1;32m   1213\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m compute_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124muse_mm_for_euclid_dist_if_necessary\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m-> 1214\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcdist\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[1;32m   1215\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m compute_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124muse_mm_for_euclid_dist\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m   1216\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _VF\u001B[38;5;241m.\u001B[39mcdist(x1, x2, p, \u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(g.weights + n.weights,lr = 0.0001)\n",
    "\n",
    "for i in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    output = n.forward(g.forward(adj,node_sig))\n",
    "    loss = lossF(output)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "disMat = torch.cdist(output,output)\n",
    "sameType = disMat * label_mat\n",
    "diffType = disMat * (1-label_mat)\n",
    "sameTypeMean = torch.sum(sameType) / 12000\n",
    "diffTypeMean = torch.sum(diffType) /348000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.1066, grad_fn=<DivBackward0>)"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sameTypeMean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.1274, grad_fn=<DivBackward0>)"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffTypeMean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.0000,  1.0000, -1.0000,  ...,  1.0000,  1.0000, -1.0000],\n        [ 1.0000,  1.0000, -1.0000,  ...,  1.0000,  1.0000, -1.0000],\n        [ 1.0000,  1.0000, -1.0000,  ...,  1.0000,  1.0000, -1.0000],\n        ...,\n        [ 1.0000,  1.0000, -1.0000,  ...,  1.0000,  1.0000, -1.0000],\n        [ 1.0000,  1.0000, -1.0000,  ...,  1.0000,  1.0000, -1.0000],\n        [ 1.0000,  1.0000, -1.0000,  ...,  1.0000,  1.0000, -1.0000]],\n       grad_fn=<TanhBackward0>)"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         ...,\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292]],\n\n        [[ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         ...,\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292]],\n\n        [[ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         ...,\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292]],\n\n        ...,\n\n        [[ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         ...,\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292]],\n\n        [[ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         ...,\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292]],\n\n        [[ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         ...,\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292],\n         [ 0.5399,  0.4505, -0.7455,  ...,  0.5693,  0.5825, -0.5292]]],\n       grad_fn=<TanhBackward0>)"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.forward(adj,node_sig)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mat = np.empty([30,30])\n",
    "plt.matshow(mat)\n",
    "self_mean = np.mean(compSelf)\n",
    "self_std = np.std(compSelf)\n",
    "others_mean = np.mean(compOthers)\n",
    "others_std = np.std(compOthers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "c1 = 10\n",
    "c2 = 20\n",
    "c3 = 30\n",
    "\n",
    "weights_1 = torch.rand(size = [1,c1], requires_grad= True)\n",
    "weights_2 = torch.rand([c1,c2],requires_grad= True)\n",
    "weights_3 = torch.rand([c2,c3], requires_grad= False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [],
   "source": [
    "def GCN(adjM,node_sigM):\n",
    "    layer_1_output = torch.matmul(torch.matmul(adjM,node_sigM),weights_1) #nxc\n",
    "    #layer 2\n",
    "    layer_2_output = torch.matmul(torch.matmul(adj,layer_1_output),weights_2) #nxc\n",
    "    #layer 2\n",
    "    layer_3_output = torch.matmul(torch.matmul(adj,layer_2_output),weights_3) #nxc\n",
    "    return layer_3_output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(439.9774, grad_fn=<MeanBackward0>)\n",
      "tensor(421.3802, grad_fn=<MeanBackward0>)\n",
      "tensor(403.1801, grad_fn=<MeanBackward0>)\n",
      "tensor(385.3835, grad_fn=<MeanBackward0>)\n",
      "tensor(367.9957, grad_fn=<MeanBackward0>)\n",
      "tensor(351.0208, grad_fn=<MeanBackward0>)\n",
      "tensor(334.4612, grad_fn=<MeanBackward0>)\n",
      "tensor(318.3172, grad_fn=<MeanBackward0>)\n",
      "tensor(302.5864, grad_fn=<MeanBackward0>)\n",
      "tensor(287.2630, grad_fn=<MeanBackward0>)\n",
      "tensor(272.3385, grad_fn=<MeanBackward0>)\n",
      "tensor(257.8025, grad_fn=<MeanBackward0>)\n",
      "tensor(243.6440, grad_fn=<MeanBackward0>)\n",
      "tensor(229.8531, grad_fn=<MeanBackward0>)\n",
      "tensor(216.4211, grad_fn=<MeanBackward0>)\n",
      "tensor(203.3407, grad_fn=<MeanBackward0>)\n",
      "tensor(190.6051, grad_fn=<MeanBackward0>)\n",
      "tensor(178.2077, grad_fn=<MeanBackward0>)\n",
      "tensor(166.1418, grad_fn=<MeanBackward0>)\n",
      "tensor(154.4001, grad_fn=<MeanBackward0>)\n",
      "tensor(142.9748, grad_fn=<MeanBackward0>)\n",
      "tensor(131.8571, grad_fn=<MeanBackward0>)\n",
      "tensor(121.0378, grad_fn=<MeanBackward0>)\n",
      "tensor(110.5065, grad_fn=<MeanBackward0>)\n",
      "tensor(100.2522, grad_fn=<MeanBackward0>)\n",
      "tensor(90.2631, grad_fn=<MeanBackward0>)\n",
      "tensor(80.5265, grad_fn=<MeanBackward0>)\n",
      "tensor(71.0291, grad_fn=<MeanBackward0>)\n",
      "tensor(61.7568, grad_fn=<MeanBackward0>)\n",
      "tensor(52.6948, grad_fn=<MeanBackward0>)\n",
      "tensor(43.8281, grad_fn=<MeanBackward0>)\n",
      "tensor(35.1407, grad_fn=<MeanBackward0>)\n",
      "tensor(26.6167, grad_fn=<MeanBackward0>)\n",
      "tensor(18.2395, grad_fn=<MeanBackward0>)\n",
      "tensor(9.9925, grad_fn=<MeanBackward0>)\n",
      "tensor(1.8588, grad_fn=<MeanBackward0>)\n",
      "tensor(-6.1785, grad_fn=<MeanBackward0>)\n",
      "tensor(-14.1364, grad_fn=<MeanBackward0>)\n",
      "tensor(-22.0319, grad_fn=<MeanBackward0>)\n",
      "tensor(-29.8819, grad_fn=<MeanBackward0>)\n",
      "tensor(-37.7034, grad_fn=<MeanBackward0>)\n",
      "tensor(-45.5130, grad_fn=<MeanBackward0>)\n",
      "tensor(-53.3276, grad_fn=<MeanBackward0>)\n",
      "tensor(-61.1637, grad_fn=<MeanBackward0>)\n",
      "tensor(-69.0377, grad_fn=<MeanBackward0>)\n",
      "tensor(-76.9660, grad_fn=<MeanBackward0>)\n",
      "tensor(-84.9648, grad_fn=<MeanBackward0>)\n",
      "tensor(-93.0503, grad_fn=<MeanBackward0>)\n",
      "tensor(-101.2383, grad_fn=<MeanBackward0>)\n",
      "tensor(-109.5449, grad_fn=<MeanBackward0>)\n",
      "tensor(-117.9855, grad_fn=<MeanBackward0>)\n",
      "tensor(-126.5759, grad_fn=<MeanBackward0>)\n",
      "tensor(-135.3314, grad_fn=<MeanBackward0>)\n",
      "tensor(-144.2675, grad_fn=<MeanBackward0>)\n",
      "tensor(-153.3993, grad_fn=<MeanBackward0>)\n",
      "tensor(-162.7419, grad_fn=<MeanBackward0>)\n",
      "tensor(-172.3102, grad_fn=<MeanBackward0>)\n",
      "tensor(-182.1190, grad_fn=<MeanBackward0>)\n",
      "tensor(-192.1831, grad_fn=<MeanBackward0>)\n",
      "tensor(-202.5170, grad_fn=<MeanBackward0>)\n",
      "tensor(-213.1351, grad_fn=<MeanBackward0>)\n",
      "tensor(-224.0519, grad_fn=<MeanBackward0>)\n",
      "tensor(-235.2814, grad_fn=<MeanBackward0>)\n",
      "tensor(-246.8379, grad_fn=<MeanBackward0>)\n",
      "tensor(-258.7353, grad_fn=<MeanBackward0>)\n",
      "tensor(-270.9875, grad_fn=<MeanBackward0>)\n",
      "tensor(-283.6082, grad_fn=<MeanBackward0>)\n",
      "tensor(-296.6109, grad_fn=<MeanBackward0>)\n",
      "tensor(-310.0094, grad_fn=<MeanBackward0>)\n",
      "tensor(-323.8168, grad_fn=<MeanBackward0>)\n",
      "tensor(-338.0464, grad_fn=<MeanBackward0>)\n",
      "tensor(-352.7115, grad_fn=<MeanBackward0>)\n",
      "tensor(-367.8248, grad_fn=<MeanBackward0>)\n",
      "tensor(-383.3992, grad_fn=<MeanBackward0>)\n",
      "tensor(-399.4475, grad_fn=<MeanBackward0>)\n",
      "tensor(-415.9821, grad_fn=<MeanBackward0>)\n",
      "tensor(-433.0157, grad_fn=<MeanBackward0>)\n",
      "tensor(-450.5603, grad_fn=<MeanBackward0>)\n",
      "tensor(-468.6283, grad_fn=<MeanBackward0>)\n",
      "tensor(-487.2315, grad_fn=<MeanBackward0>)\n",
      "tensor(-506.3821, grad_fn=<MeanBackward0>)\n",
      "tensor(-526.0918, grad_fn=<MeanBackward0>)\n",
      "tensor(-546.3723, grad_fn=<MeanBackward0>)\n",
      "tensor(-567.2352, grad_fn=<MeanBackward0>)\n",
      "tensor(-588.6923, grad_fn=<MeanBackward0>)\n",
      "tensor(-610.7548, grad_fn=<MeanBackward0>)\n",
      "tensor(-633.4343, grad_fn=<MeanBackward0>)\n",
      "tensor(-656.7421, grad_fn=<MeanBackward0>)\n",
      "tensor(-680.6898, grad_fn=<MeanBackward0>)\n",
      "tensor(-705.2886, grad_fn=<MeanBackward0>)\n",
      "tensor(-730.5502, grad_fn=<MeanBackward0>)\n",
      "tensor(-756.4858, grad_fn=<MeanBackward0>)\n",
      "tensor(-783.1068, grad_fn=<MeanBackward0>)\n",
      "tensor(-810.4249, grad_fn=<MeanBackward0>)\n",
      "tensor(-838.4518, grad_fn=<MeanBackward0>)\n",
      "tensor(-867.1990, grad_fn=<MeanBackward0>)\n",
      "tensor(-896.6787, grad_fn=<MeanBackward0>)\n",
      "tensor(-926.9025, grad_fn=<MeanBackward0>)\n",
      "tensor(-957.8828, grad_fn=<MeanBackward0>)\n",
      "tensor(-989.6315, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = optim.Adam([weights_1,weights_2,weights_3],lr = 0.01)\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = GCN(adj,node_sig)\n",
    "    #output = torch.mean(weights_1)\n",
    "    loss = torch.mean(output)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1., 1., 1.])"
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([450,500,300])\n",
    "torch.tanh(a)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "# adj_tensor = torch.tensor(adjMats)\n",
    "# edge_indx = adj_tensor.nonzero().t().contiguous()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [],
   "source": [
    "# # print(edge_indx)\n",
    "# # print(edge_indx.shape)\n",
    "# print(torch.max(edge_indx))\n",
    "# print(edge_indx.size())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [],
   "source": [
    "# data = torch_geometric.data.Data(x = torch.zeros(252,1), edge_index=edge_indx)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [],
   "source": [
    "# #hyper parameters\n",
    "# in_channels = 252\n",
    "# out_channels = 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "# #set up gnn\n",
    "# model = Sequential('x, edge_index', [\n",
    "#     (GCNConv(in_channels, 64), 'x, edge_index -> x'),\n",
    "#     ReLU(inplace=True),\n",
    "#     (GCNConv(64, 64), 'x, edge_index -> x'),\n",
    "#     ReLU(inplace=True),\n",
    "#     Linear(64, out_channels),\n",
    "# ])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [],
   "source": [
    "# data = torch.tensor(adjMats).float()\n",
    "#\n",
    "# model(data,edge_indx)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#run gnn, get (NxC) matrix\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#compute contrastive loss\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#update weights (FxC)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}