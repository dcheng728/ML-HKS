{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "%run helpers_preproc.ipynb\n",
    "%run alternate_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "origDataDir = 'SHREC11/'\n",
    "newDataDir = 'SHREC11_plus/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate additional data and update label\n",
    "origLabel = np.array(readLbl(600,origDataDir + 'labels.txt'),dtype = int) #first take in the original labels\n",
    "newLabelFile = open(newDataDir + \"labels.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "radiuss = [0.0002,0.0004,0.0008,0.0016]\n",
    "\n",
    "for i in range(600):\n",
    "    #First create the new obj file\n",
    "    origN = origDataDir + 'T' + str(i) + '.obj'\n",
    "    \n",
    "    for j in range(len(radiuss)):\n",
    "        radius = radiuss[j]\n",
    "        newN = newDataDir + 'T' + str(i+600*(j)) + '.obj'\n",
    "    \n",
    "        #newLabelFile.write(\"T\" + str(i) + \".obj\" + \" \" + str(origLabel[i]) + \"\\n\")\n",
    "        if not (write_new_files(origN,newN,radius)):\n",
    "            print('Data generation failed on ' , i)\n",
    "        \n",
    "        newLabelFile.write(\"T\" + str(i + 600*(j)) + \".obj\" + \" \" + str(origLabel[i]) + \"\\n\")\n",
    "\n",
    "    \n",
    "newLabelFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the data, get normalized adjacency (NxN)\n",
    "num_meshes = 600 * 4\n",
    "train_size = 450 * 4\n",
    "test_size = 150 * 4\n",
    "mesh_dir = 'SHREC11_plus/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_swaps = 10000\n",
    "\n",
    "labels = np.array(readLbl(num_meshes,mesh_dir+'labels.txt'))\n",
    "\n",
    "for swap_idx in range(num_swaps):\n",
    "    idxA = random.randint(0,num_meshes-1)\n",
    "    idxB = random.randint(0,num_meshes-1)\n",
    "    \n",
    "    if idxA != idxB:\n",
    "        labelA = labels[idxA]\n",
    "        labelB = labels[idxB]\n",
    "        labels[idxA] = labelB\n",
    "        labels[idxB] = labelA\n",
    "\n",
    "        #change filenames\n",
    "        os.rename(mesh_dir+'T' + str(idxA) + '.obj',mesh_dir+'temp.obj')\n",
    "        os.rename(mesh_dir+'T' + str(idxB) + '.obj',mesh_dir+'T' + str(idxA) + '.obj')\n",
    "        os.rename(mesh_dir+'temp.obj',mesh_dir+'T' + str(idxB) + '.obj')\n",
    "\n",
    "newLabelFile = open(newDataDir + \"labels.txt\", \"w\")\n",
    "for i in range(num_meshes):\n",
    "        newLabelFile.write(\"T\" + str(i) + \".obj\" + \" \" + str(labels[i]) + \"\\n\")\n",
    "\n",
    "\n",
    "newLabelFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_np = np.array(readLbl(num_meshes,mesh_dir+'labels.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_sigs_list = []\n",
    "normed_adjMats_list = []\n",
    "\n",
    "for i in range(num_meshes):\n",
    "    fName = 'T' + str(i) + '.obj'\n",
    "    adj_noscale_read = obj_2_adj_noscale(mesh_dir + fName)\n",
    "    adj_scaled_read = obj_2_adj(mesh_dir + fName)\n",
    "\n",
    "    if (adj_scaled_read.shape[0] < 252):\n",
    "        adj_noscale = np.empty([252,252])\n",
    "        adj_scaled = np.empty([252,252])\n",
    "    else:\n",
    "        adj_noscale = adj_noscale_read\n",
    "        adj_scaled = adj_scaled_read\n",
    "\n",
    "    adj_normalized = adj_noscale / np.reshape(np.sum(adj_noscale,axis = 1),[adj_noscale.shape[0],1])\n",
    "\n",
    "    #node level signal extraction\n",
    "    node_degs = np.sum(adj_noscale,axis = 0)\n",
    "    node_neigh_max = np.max(adj_scaled,axis = 0)\n",
    "    node_neigh_min = np.min(adj_scaled,axis = 0)\n",
    "    node_neigh_sum = np.sum(adj_scaled,axis = 0)\n",
    "    node_neigh_mean = np.sum(adj_scaled,axis = 0)\n",
    "\n",
    "    node_sig = np.stack([node_degs,node_neigh_max,node_neigh_min,node_neigh_sum,node_neigh_mean],axis = 1)\n",
    "    \n",
    "    node_sigs_list.append(node_sig)\n",
    "    normed_adjMats_list.append(adj_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_adjMats = np.nan_to_num(np.stack(normed_adjMats_list))\n",
    "node_sigs = np.nan_to_num(np.stack(node_sigs_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_mean = np.reshape(np.mean(node_sigs,axis = 1),[num_meshes,1,5])\n",
    "#node_sigs = node_sigs - feature_mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[4.        , 0.13550079, 0.        , 0.42369974, 0.42369974],\n",
       "        [7.        , 0.17212504, 0.        , 0.98396842, 0.98396842],\n",
       "        [5.        , 0.16103479, 0.        , 0.68239826, 0.68239826],\n",
       "        ...,\n",
       "        [7.        , 0.19047461, 0.        , 1.03865331, 1.03865331],\n",
       "        [4.        , 0.16742295, 0.        , 0.53104589, 0.53104589],\n",
       "        [5.        , 0.294856  , 0.        , 0.85576911, 0.85576911]],\n",
       "\n",
       "       [[4.        , 0.17414296, 0.        , 0.43763473, 0.43763473],\n",
       "        [5.        , 0.15670403, 0.        , 0.62843306, 0.62843306],\n",
       "        [5.        , 0.17932172, 0.        , 0.68915266, 0.68915266],\n",
       "        ...,\n",
       "        [7.        , 0.25971837, 0.        , 1.00513952, 1.00513952],\n",
       "        [6.        , 0.14572146, 0.        , 0.55284372, 0.55284372],\n",
       "        [7.        , 0.1736732 , 0.        , 0.95802574, 0.95802574]],\n",
       "\n",
       "       [[5.        , 0.08813521, 0.        , 0.3568331 , 0.3568331 ],\n",
       "        [7.        , 0.10132539, 0.        , 0.46379741, 0.46379741],\n",
       "        [6.        , 0.09283365, 0.        , 0.44004323, 0.44004323],\n",
       "        ...,\n",
       "        [7.        , 0.11377004, 0.        , 0.54189935, 0.54189935],\n",
       "        [6.        , 0.1353866 , 0.        , 0.49690803, 0.49690803],\n",
       "        [7.        , 0.1353866 , 0.        , 0.52555522, 0.52555522]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[7.        , 0.17720654, 0.        , 0.88234615, 0.88234615],\n",
       "        [6.        , 0.19396593, 0.        , 0.68010108, 0.68010108],\n",
       "        [6.        , 0.19189459, 0.        , 0.67044472, 0.67044472],\n",
       "        ...,\n",
       "        [5.        , 0.12989205, 0.        , 0.55099438, 0.55099438],\n",
       "        [5.        , 0.17090125, 0.        , 0.5944272 , 0.5944272 ],\n",
       "        [5.        , 0.12775089, 0.        , 0.43364102, 0.43364102]],\n",
       "\n",
       "       [[6.        , 0.16285427, 0.        , 0.49353809, 0.49353809],\n",
       "        [4.        , 0.07617924, 0.        , 0.17411704, 0.17411704],\n",
       "        [7.        , 0.12634109, 0.        , 0.58185435, 0.58185435],\n",
       "        ...,\n",
       "        [5.        , 0.07708077, 0.        , 0.33922665, 0.33922665],\n",
       "        [6.        , 0.08341328, 0.        , 0.39129799, 0.39129799],\n",
       "        [5.        , 0.08016382, 0.        , 0.32484858, 0.32484858]],\n",
       "\n",
       "       [[3.        , 0.05529031, 0.        , 0.15317593, 0.15317593],\n",
       "        [8.        , 0.14827533, 0.        , 0.73185359, 0.73185359],\n",
       "        [4.        , 0.08974028, 0.        , 0.29265898, 0.29265898],\n",
       "        ...,\n",
       "        [5.        , 0.14441056, 0.        , 0.47902876, 0.47902876],\n",
       "        [6.        , 0.14541408, 0.        , 0.56423314, 0.56423314],\n",
       "        [4.        , 0.09162055, 0.        , 0.27969877, 0.27969877]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_sigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_adjMats_tr = normed_adjMats[:train_size,:,:]\n",
    "node_sigs_tr = node_sigs[:train_size,:,:]\n",
    "label_np_tr = label_np[:train_size]\n",
    "\n",
    "normed_adjMats_ts = normed_adjMats[train_size:,:,:]\n",
    "node_sigs_ts = node_sigs[train_size:,:,:]\n",
    "label_np_ts = label_np[train_size:]\n",
    "\n",
    "label_mat_tr = torch.tensor(np.where(igl.all_pairs_distances(label_np_tr,label_np_tr,False) > 0.5,0,1)).float()\n",
    "label_mat_ts = torch.tensor(np.where(igl.all_pairs_distances(label_np_ts,label_np_ts,False) > 0.5,0,1)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_adjMats_tr = torch.tensor(normed_adjMats_tr).float()\n",
    "node_sigs_tr = torch.tensor(node_sigs_tr).float()\n",
    "label_tr = torch.tensor(label_np_tr).float()\n",
    "\n",
    "normed_adjMats_ts = torch.tensor(normed_adjMats_ts).float()\n",
    "node_sigs_ts = torch.tensor(node_sigs_ts).float()\n",
    "label_ts = torch.tensor(label_np_ts).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_mat_train = form_label_matrix(label_mat, train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normed_adjMats_list_train, node_sigs_list_train = prep_data(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normed_adjMats_train = np.stack(normed_adjMats_list_train)\n",
    "#node_sigs_train = np.stack(node_sigs_list_train)\n",
    "\n",
    "#normed_adjMats_train = torch.tensor(normed_adjMats_train,requires_grad=False).float()\n",
    "#node_sigs_train = torch.tensor(node_sigs_train,requires_grad=False).float()\n",
    "\n",
    "#normed_adjMats_train = torch.nan_to_num(normed_adjMats_train,0,0,0)\n",
    "#node_sigs_train = torch.nan_to_num(node_sigs_train,0,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1800, 252, 5])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_sigs_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1800, 252, 5])\n",
      "torch.Size([1800, 5])\n"
     ]
    }
   ],
   "source": [
    "%run NNs.ipynb\n",
    "g = GCN(5,[5,5,5,5])\n",
    "n = MLP(252*5,[5,5,5,5])\n",
    "output1 = g.forward(normed_adjMats_tr,node_sigs_tr)\n",
    "output2 = n.forward(output1)\n",
    "print(output1.shape)\n",
    "print(output2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lossF(features,lbl_mat):\n",
    "    disMat = torch.cdist(features,features)\n",
    "    sameType = disMat * (lbl_mat-torch.diag(torch.ones(lbl_mat.shape[0])))\n",
    "    diffType = disMat * (1-lbl_mat)\n",
    "\n",
    "    diffTypeScaled = diffType * torch.exp(-diffType)\n",
    "    diffTypeScaledMean = torch.sum(diffTypeScaled)/ torch.count_nonzero(diffTypeScaled)\n",
    "    diffTypeMean = torch.sum(diffType)/ torch.count_nonzero(diffType)\n",
    "\n",
    "    sameTypeMean = torch.sum(sameType) / torch.count_nonzero(sameType)\n",
    "    sameTypeStd = torch.sum((sameType - sameTypeMean)**2) / torch.count_nonzero(sameType)\n",
    "\n",
    "    #toRet = -(sameTypeMean-diffTypeMean)**2 # + 0.01 * torch.sqrt(sameTypeStd)\n",
    "    #toRet = -(sameTypeMean-diffTypeMean)**2 #+ 0.5*(sameTypeMean + 0.01 * torch.sqrt(sameTypeStd))\n",
    "    toRet = -(sameTypeMean-diffTypeMean)**2 - diffTypeScaledMean + 0.5 * torch.sqrt(sameTypeStd)\n",
    "    \n",
    "    print(sameTypeMean.detach().numpy(),diffTypeMean.detach().numpy(),torch.sqrt(sameTypeStd).detach().numpy(), toRet.detach().numpy())\n",
    "    return  toRet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009815609 0.011271889 0.05397419 0.015909836\n",
      "0.009717828 0.011211275 0.05336741 0.015658468\n",
      "0.009472763 0.011008783 0.052121464 0.015239498\n",
      "0.00926116 0.010752921 0.05092417 0.014887024\n",
      "0.009374454 0.010783215 0.0519943 0.015427266\n",
      "0.009049778 0.010476916 0.04978067 0.014581689\n",
      "0.009061721 0.010508302 0.049872242 0.014601457\n",
      "0.008726695 0.010164029 0.04800009 0.013994484\n",
      "0.008963561 0.010384583 0.049749646 0.01469036\n",
      "0.008635034 0.010129281 0.047522508 0.013792751\n",
      "0.0087919375 0.010210031 0.048792332 0.014378063\n",
      "0.008653564 0.010141754 0.04764602 0.013844778\n",
      "0.008573455 0.009995102 0.047163233 0.013741373\n",
      "0.00846559 0.0099236695 0.04686252 0.013680123\n",
      "0.008201751 0.009648194 0.045134146 0.013063062\n",
      "0.008094112 0.009483714 0.04454387 0.012926587\n",
      "0.0080969 0.00954114 0.044558536 0.012879364\n",
      "0.00816806 0.009536965 0.04517312 0.013206965\n",
      "0.007848002 0.009199285 0.043153428 0.012506372\n",
      "0.007963789 0.009263649 0.04385141 0.012797758\n",
      "0.007758826 0.009096805 0.042682566 0.012371682\n",
      "0.007766119 0.009109668 0.042729642 0.012383042\n",
      "0.0076693324 0.008979276 0.04218223 0.012236233\n",
      "0.0076539163 0.008982188 0.042115096 0.012201496\n",
      "0.0075407126 0.008900641 0.041536305 0.011992176\n",
      "0.0073945713 0.008705276 0.040694077 0.011759815\n",
      "0.0073113646 0.008632867 0.04024406 0.011605794\n",
      "0.0072675175 0.008618198 0.04001325 0.0115054\n",
      "0.0070941485 0.008433334 0.039067313 0.0112118935\n",
      "0.0071013365 0.0083773155 0.03911411 0.011291405\n",
      "0.007084174 0.008379467 0.039029617 0.0112465415\n",
      "0.0069483947 0.008194049 0.03825124 0.011036527\n",
      "0.00703075 0.00825865 0.03871455 0.011206344\n",
      "0.006847103 0.0081119845 0.037686106 0.010833043\n",
      "0.0068497425 0.008076231 0.03770589 0.010879022\n",
      "0.006778823 0.008046856 0.037336018 0.010723411\n",
      "0.0068128156 0.008018846 0.037496317 0.010829966\n",
      "0.006744787 0.007948028 0.03741969 0.0108775515\n",
      "0.006638181 0.007879283 0.03675898 0.010610171\n",
      "0.0064710164 0.007661097 0.035644308 0.010254729\n",
      "0.0065184473 0.007697768 0.035909604 0.010351691\n",
      "0.0063779554 0.0075384756 0.035139356 0.0101212645\n",
      "0.006342044 0.0075194817 0.034941997 0.010041831\n",
      "0.006243187 0.0073752073 0.034385018 0.009902719\n",
      "0.006245587 0.0073920265 0.03441447 0.009902174\n",
      "0.0061587547 0.0072587007 0.033928845 0.009789181\n",
      "0.0061331936 0.0072460943 0.034075495 0.009890521\n",
      "0.006169263 0.0072556203 0.03442581 0.010064997\n",
      "0.005921791 0.007054754 0.032641128 0.009345053\n",
      "0.0059995367 0.007139117 0.03308124 0.009483479\n",
      "0.005969957 0.007079552 0.032897383 0.009448945\n",
      "0.005907053 0.0069855843 0.03256102 0.009373538\n",
      "0.005816382 0.006896689 0.032042887 0.009200446\n",
      "0.0059043122 0.006976662 0.03253934 0.009370679\n",
      "0.0057664844 0.0068430835 0.031783756 0.009124147\n",
      "0.005679873 0.00676962 0.0312987 0.008952524\n",
      "0.0056346394 0.006677357 0.031059317 0.0089237075\n",
      "0.0055295285 0.0065754247 0.030477075 0.008732116\n",
      "0.0056564324 0.0066756005 0.031178862 0.00898532\n",
      "0.0056072986 0.00663061 0.031151425 0.009028624\n",
      "0.0055054566 0.00653709 0.030359117 0.008711636\n",
      "0.005448201 0.006453467 0.03004103 0.008634508\n",
      "0.0054103318 0.006404053 0.029839478 0.008582275\n",
      "0.0054927566 0.0064608827 0.030716246 0.008985778\n",
      "0.005336141 0.006329042 0.029437805 0.00845547\n",
      "0.0052705016 0.0062645595 0.029096287 0.008348629\n",
      "0.0052630506 0.0062154974 0.029038109 0.00836733\n",
      "0.0052065114 0.006179107 0.028725037 0.008245992\n",
      "0.005191675 0.0061363834 0.02863975 0.008245101\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m output \u001b[38;5;241m=\u001b[39m n\u001b[38;5;241m.\u001b[39mforward(g\u001b[38;5;241m.\u001b[39mforward(normed_adjMats_tr,node_sigs_tr))\n\u001b[1;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m lossF(output,label_mat_tr)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.conda/envs/TSM/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/TSM/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(g.weights + n.weights,lr = 0.0001)\n",
    "for i in range(100000):\n",
    "    optimizer.zero_grad()\n",
    "    output = n.forward(g.forward(normed_adjMats_tr,node_sigs_tr))\n",
    "    loss = lossF(output,label_mat_tr)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = n.forward(g.forward(normed_adjMats_tr,node_sigs_tr))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disMat = torch.cdist(output,output).flatten()\n",
    "mask_same = torch.tensor(label_mat_tr - torch.diag(torch.ones(train_size)),dtype=bool).flatten()\n",
    "mask_diff = torch.tensor(1 - label_mat_tr,dtype=bool).flatten()\n",
    "sameComp = disMat[mask_same].detach().numpy()\n",
    "diffComp = disMat[mask_diff].detach().numpy()\n",
    "weightsSame = np.ones_like(sameComp) / len(sameComp)\n",
    "weightsDiff = np.ones_like(diffComp) / len(diffComp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(sameComp),np.mean(diffComp))\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "ax1.hist(sameComp,bins=np.arange(0,np.max(sameComp),0.01),weights=weightsSame,log = False)\n",
    "ax1.set_title('Same Comp')\n",
    "ax2.hist(diffComp,bins=np.arange(0,np.max(diffComp),0.01),weights=weightsDiff,log = False)\n",
    "ax2.set_title('Diff Comp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = n.forward(g.forward(normed_adjMats_ts,node_sigs_ts))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disMat = torch.cdist(output,output).flatten()\n",
    "mask_same = torch.tensor(label_mat_ts - torch.diag(torch.ones(test_size)),dtype=bool).flatten()\n",
    "mask_diff = torch.tensor(1 - label_mat_ts,dtype=bool).flatten()\n",
    "sameComp = disMat[mask_same].detach().numpy()\n",
    "diffComp = disMat[mask_diff].detach().numpy()\n",
    "weightsSame = np.ones_like(sameComp) / len(sameComp)\n",
    "weightsDiff = np.ones_like(diffComp) / len(diffComp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(sameComp),np.mean(diffComp))\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "ax1.hist(sameComp,bins=np.arange(0,np.max(sameComp),0.01),weights=weightsSame,log = False)\n",
    "ax1.set_title('Same Comp')\n",
    "ax2.hist(diffComp,bins=np.arange(0,np.max(diffComp),0.01),weights=weightsDiff,log = False)\n",
    "ax2.set_title('Diff Comp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from matplotlib.pyplot import figure\n",
    "#figure(figsize=(10, 8), dpi=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BigDiffIndices = np.where(disMat.detach().numpy().flatten() > 0.75)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(BigDiffIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vizMat = np.zeros([30,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating 30 categories into less categories\n",
    "\n",
    "# for index in BigDiffIndices:\n",
    "#     row, col = divmod(index,test_size)\n",
    "#     cat1, cat2 = int(label_np_ts[row]),int(label_np_ts[col])\n",
    "#     if cat1 == cat2:\n",
    "#         print(cat1, cat2)\n",
    "#     vizMat[cat1,cat2] += 1\n",
    "#     vizMat[cat2,cat1] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vizMat = vizMat / np.max(vizMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(vizMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = output.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "newX = pca.fit_transform(features)\n",
    "np.sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_show = [0,1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPlot = []\n",
    "for i in range(len(indices_to_show)):\n",
    "    a = newX[np.where(label_np_ts == indices_to_show[i]),:]\n",
    "    a = np.reshape(a,[a.shape[1],a.shape[2]])\n",
    "    categories = np.ones([a.shape[0],1]) * indices_to_show[i]\n",
    "    b = np.concatenate((a,categories),axis = 1)\n",
    "    dataPlot.append(b)\n",
    "    \n",
    "dataPlot = np.concatenate(dataPlot,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPlot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.scatter(dataPlot[:,0], dataPlot[:,1], dataPlot[:,2], label=dataPlot[:,3], c = dataPlot[:,3], cmap = 'viridis')\n",
    "#ax.axis('off')\n",
    "ax.axes.xaxis.set_ticklabels([])\n",
    "ax.axes.yaxis.set_ticklabels([])\n",
    "ax.axes.zaxis.set_ticklabels([])\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.set_title('[0,1,2,3,4]')\n",
    "\n",
    "#plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
