{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%run helpers.ipynb\n",
    "%run NN.ipynb\n",
    "np.set_printoptions(precision=2)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hyper parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "canvas_width = 51\n",
    "canvas_height= 51\n",
    "center = np.array([(canvas_height-1) / 2, (canvas_width-1) / 2])\n",
    "num_drawings = 100\n",
    "num_samples = 200\n",
    "bins = np.arange(0,21)\n",
    "\n",
    "ts = np.array([0.17,0.2,0.08,0.1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "canvass_train = np.zeros([num_drawings,canvas_height, canvas_width])\n",
    "canvass_test = np.zeros([num_drawings,canvas_height, canvas_width])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "for i in range(num_drawings):\n",
    "    if np.random.rand() > 0.5:\n",
    "        canvass_train[i] = draw_polygon(canvass_train[2],3,5)\n",
    "    else:\n",
    "        canvass_train[i] = draw_ellipse(canvass_train[i],10,15)\n",
    "\n",
    "for i in range(num_drawings):\n",
    "    if np.random.rand() > 0.5:\n",
    "        canvass_test[i] = draw_polygon(canvass_test[2],3,5)\n",
    "    else:\n",
    "        canvass_test[i] = draw_ellipse(canvass_test[i],10,15)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get the length vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "lengthss_train = get_length_encoding(canvass_train,num_samples,bins)\n",
    "lengthss_test = get_length_encoding(canvass_test,num_samples,bins)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Turn image into graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<class 'networkx.utils.decorators.argmap'> compilation 8:4: FutureWarning: laplacian_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "<class 'networkx.utils.decorators.argmap'> compilation 8:4: FutureWarning: laplacian_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n"
     ]
    }
   ],
   "source": [
    "lap_mats_train = adj_mat_to_lap_mat(imgs_to_adj_mat(canvass_train))\n",
    "lap_mats_test = adj_mat_to_lap_mat(imgs_to_adj_mat(canvass_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compute eigenvalues & eigenvectors &hks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "evals_train,Us_train = lap_mat_to_eigen(lap_mats_train)\n",
    "hkss_train = batch_heat_kernel_signature(ts, evals_train, Us_train)\n",
    "\n",
    "evals_test,Us_test = lap_mat_to_eigen(lap_mats_test)\n",
    "hkss_test = batch_heat_kernel_signature(ts, evals_test, Us_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set up Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class HeatKernelSignatureNet(nn.Module):\n",
    "    #takes in image (51 x 51), returns heat kernel signature at [0.17,0.47,0.58,0.81]\n",
    "\n",
    "    def __init__(self,image_width,image_height):\n",
    "        super().__init__()\n",
    "        self.image_width = image_width\n",
    "        self.image_height = image_height\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3)\n",
    "\n",
    "        self.MLP = MLP(48,4,10,100)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.tanh(self.conv1(x)))\n",
    "        x = self.pool(torch.tanh(self.conv2(x)))\n",
    "        x = self.pool(torch.tanh(self.conv3(x)))\n",
    "\n",
    "        x = torch.flatten(x,start_dim=1,end_dim=-1)\n",
    "        x = self.MLP(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "HeatKernelSignatureNet(\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1))\n  (conv2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n  (conv3): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n  (MLP): MLP(\n    (model): Sequential(\n      (fc1): Linear(in_features=48, out_features=100, bias=True)\n      (tanh1): Tanh()\n      (fc2): Linear(in_features=100, out_features=100, bias=True)\n      (tanh2): Tanh()\n      (fc3): Linear(in_features=100, out_features=100, bias=True)\n      (tanh3): Tanh()\n      (fc4): Linear(in_features=100, out_features=100, bias=True)\n      (tanh4): Tanh()\n      (fc5): Linear(in_features=100, out_features=100, bias=True)\n      (tanh5): Tanh()\n      (fc6): Linear(in_features=100, out_features=100, bias=True)\n      (tanh6): Tanh()\n      (fc7): Linear(in_features=100, out_features=100, bias=True)\n      (tanh7): Tanh()\n      (fc8): Linear(in_features=100, out_features=100, bias=True)\n      (tanh8): Tanh()\n      (fc9): Linear(in_features=100, out_features=100, bias=True)\n      (tanh9): Tanh()\n      (fc10): Linear(in_features=100, out_features=4, bias=True)\n    )\n  )\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hksNet = HeatKernelSignatureNet(51,51)\n",
    "hksNet.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "canvass_train_torch = torch.tensor(canvass_train).to(device).float()\n",
    "canvass_train_torch = torch.reshape(canvass_train_torch,[num_drawings,1,canvas_width,canvas_height])\n",
    "\n",
    "canvass_test_torch = torch.tensor(canvass_test).to(device).float()\n",
    "canvass_test_torch = torch.reshape(canvass_test_torch,[num_drawings,1,canvas_width,canvas_height])\n",
    "\n",
    "hkss_train_torch = torch.tensor(hkss_train).to(device).float()\n",
    "hkss_test_torch = torch.tensor(hkss_test).to(device).float()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "learning_rates = [0.005] * 2000 + [0.0005] * 2000 + [0.00005] * 2000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0577194690704346\n",
      "10 0.11155016720294952\n",
      "20 0.10844852775335312\n",
      "30 0.11175420135259628\n",
      "40 0.12559448182582855\n",
      "50 0.13086269795894623\n",
      "60 0.14109867811203003\n",
      "70 0.15023352205753326\n",
      "80 0.10464713722467422\n",
      "90 0.08999171853065491\n",
      "100 0.11308469623327255\n",
      "110 0.0793231800198555\n",
      "120 0.01574951782822609\n",
      "130 0.16162581741809845\n",
      "140 0.1151495948433876\n",
      "150 0.029542239382863045\n",
      "160 0.009614511393010616\n",
      "170 0.015491340309381485\n",
      "180 0.016168462112545967\n",
      "190 0.02346944622695446\n",
      "200 0.06915082782506943\n",
      "210 0.22496938705444336\n",
      "220 0.13043813407421112\n",
      "230 0.05652587115764618\n",
      "240 0.1345088928937912\n",
      "250 0.10228119045495987\n",
      "260 0.09078903496265411\n",
      "270 0.07976115494966507\n",
      "280 0.13409122824668884\n",
      "290 0.05820099264383316\n",
      "300 0.060952797532081604\n",
      "310 0.06968194991350174\n",
      "320 0.046608228236436844\n",
      "330 0.03725931793451309\n",
      "340 0.0429488867521286\n",
      "350 0.03518855571746826\n",
      "360 0.033112019300460815\n",
      "370 0.02540845237672329\n",
      "380 0.06433960795402527\n",
      "390 0.07196997851133347\n",
      "400 0.05321565642952919\n",
      "410 0.028080826625227928\n",
      "420 0.06441875547170639\n",
      "430 0.019609032198786736\n",
      "440 0.018649093806743622\n",
      "450 0.023772912099957466\n",
      "460 0.03276746720075607\n",
      "470 0.02259197272360325\n",
      "480 0.011759771965444088\n",
      "490 0.019278233870863914\n",
      "500 0.008356476202607155\n",
      "510 0.012441419064998627\n",
      "520 0.06904160231351852\n",
      "530 0.016768336296081543\n",
      "540 0.005285337101668119\n",
      "550 0.022344404831528664\n",
      "560 0.027829403057694435\n",
      "570 0.010042903013527393\n",
      "580 0.0117286192253232\n",
      "590 0.03421860560774803\n",
      "600 0.016872648149728775\n",
      "610 0.019382350146770477\n",
      "620 0.0040972232818603516\n",
      "630 0.005483427084982395\n",
      "640 0.023742880672216415\n",
      "650 0.020506424829363823\n",
      "660 0.020463936030864716\n",
      "670 0.10008764266967773\n",
      "680 0.05307464674115181\n",
      "690 0.03644919767975807\n",
      "700 0.02243645116686821\n",
      "710 0.04256396368145943\n",
      "720 0.1386301964521408\n",
      "730 0.05858911946415901\n",
      "740 0.06576772034168243\n",
      "750 0.05797947570681572\n",
      "760 0.05426250398159027\n",
      "770 0.06120329350233078\n",
      "780 0.05960957705974579\n",
      "790 0.07673459500074387\n",
      "800 0.07169190049171448\n",
      "810 0.06482227146625519\n",
      "820 0.11543784290552139\n",
      "830 0.050620485097169876\n",
      "840 0.03978888690471649\n",
      "850 0.03942342847585678\n",
      "860 0.053761325776576996\n",
      "870 0.1326369047164917\n",
      "880 0.10706838965415955\n",
      "890 0.11112038791179657\n",
      "900 0.07916610687971115\n",
      "910 0.11017899215221405\n",
      "920 0.0965384989976883\n",
      "930 0.1420806348323822\n",
      "940 0.0858437791466713\n",
      "950 0.10417022556066513\n",
      "960 0.08254725486040115\n",
      "970 0.07987979054450989\n",
      "980 0.07140392065048218\n",
      "990 0.09129531681537628\n",
      "1000 0.0829886794090271\n",
      "1010 0.10976003855466843\n",
      "1020 0.10226120799779892\n",
      "1030 0.09684086591005325\n",
      "1040 0.141442209482193\n",
      "1050 0.0972863957285881\n",
      "1060 0.08489613234996796\n",
      "1070 0.12405694276094437\n",
      "1080 0.04786493256688118\n",
      "1090 0.05077238008379936\n",
      "1100 0.0796252116560936\n",
      "1110 0.049171075224876404\n",
      "1120 0.08706419914960861\n",
      "1130 0.13187366724014282\n",
      "1140 0.16185367107391357\n",
      "1150 0.015095633454620838\n",
      "1160 0.03822895139455795\n",
      "1170 0.04672984778881073\n",
      "1180 0.13275672495365143\n",
      "1190 0.04709775000810623\n",
      "1200 0.06640741229057312\n",
      "1210 0.026565739884972572\n",
      "1220 0.05577285587787628\n",
      "1230 0.030394239351153374\n",
      "1240 0.06057579815387726\n",
      "1250 0.06457694619894028\n",
      "1260 0.059738606214523315\n",
      "1270 0.05265501141548157\n",
      "1280 0.031811609864234924\n",
      "1290 0.044903021305799484\n",
      "1300 0.06746385246515274\n",
      "1310 0.044981297105550766\n",
      "1320 0.039972301572561264\n",
      "1330 0.08779876679182053\n",
      "1340 0.08416876196861267\n",
      "1350 0.21152442693710327\n",
      "1360 0.0930609479546547\n",
      "1370 0.07945910096168518\n",
      "1380 0.06794151663780212\n",
      "1390 0.1349230706691742\n",
      "1400 0.09766320884227753\n",
      "1410 0.2369799017906189\n",
      "1420 0.01877770945429802\n",
      "1430 0.017130138352513313\n",
      "1440 0.010332223027944565\n",
      "1450 0.02189665287733078\n",
      "1460 0.019563479349017143\n",
      "1470 0.014786171726882458\n",
      "1480 0.005892328452318907\n",
      "1490 0.00419226149097085\n",
      "1500 0.02966822125017643\n",
      "1510 0.09817272424697876\n",
      "1520 0.03263020142912865\n",
      "1530 0.02639506570994854\n",
      "1540 0.0174682829529047\n",
      "1550 0.10949106514453888\n",
      "1560 0.024135001003742218\n",
      "1570 0.03096698224544525\n",
      "1580 0.05215632915496826\n",
      "1590 0.023393338546156883\n",
      "1600 0.042364973574876785\n",
      "1610 0.07473359256982803\n",
      "1620 0.04160666465759277\n",
      "1630 0.048879340291023254\n",
      "1640 0.07077258825302124\n",
      "1650 0.03827430307865143\n",
      "1660 0.06110776960849762\n",
      "1670 0.020709490403532982\n",
      "1680 0.057021867483854294\n",
      "1690 0.04848330467939377\n",
      "1700 0.6362782120704651\n",
      "1710 0.11558632552623749\n",
      "1720 0.08513733744621277\n",
      "1730 0.13087520003318787\n",
      "1740 0.0820535197854042\n",
      "1750 0.04935399442911148\n",
      "1760 0.10595245659351349\n",
      "1770 0.08661842346191406\n",
      "1780 0.0774899423122406\n",
      "1790 0.02283458039164543\n",
      "1800 0.04176358878612518\n",
      "1810 0.05271162837743759\n",
      "1820 0.024914855137467384\n",
      "1830 0.028783541172742844\n",
      "1840 0.022516103461384773\n",
      "1850 0.01866718754172325\n",
      "1860 0.02600502036511898\n",
      "1870 0.08560826629400253\n",
      "1880 0.014917585998773575\n",
      "1890 0.033348988741636276\n",
      "1900 0.034798599779605865\n",
      "1910 0.03618708997964859\n",
      "1920 0.03584521636366844\n",
      "1930 0.014550630934536457\n",
      "1940 0.009120350703597069\n",
      "1950 0.010338032618165016\n",
      "1960 0.007499359082430601\n",
      "1970 0.07458119094371796\n",
      "1980 0.058977626264095306\n",
      "1990 0.22047023475170135\n",
      "2000 0.014230754226446152\n",
      "2010 0.000681774690747261\n",
      "2020 0.0005926892044954002\n",
      "2030 0.0005407886346802115\n",
      "2040 0.0005077713867649436\n",
      "2050 0.0004808687372133136\n",
      "2060 0.000463405332993716\n",
      "2070 0.0004493637243285775\n",
      "2080 0.0004388863453641534\n",
      "2090 0.0004308271745685488\n",
      "2100 0.00042480387492105365\n",
      "2110 0.0004192183550912887\n",
      "2120 0.0004137791402172297\n",
      "2130 0.000407851068302989\n",
      "2140 0.0004026586830150336\n",
      "2150 0.00039789106813259423\n",
      "2160 0.0003940215101465583\n",
      "2170 0.00039072733488865197\n",
      "2180 0.0003885520272888243\n",
      "2190 0.00038742166361771524\n",
      "2200 0.00038559228414669633\n",
      "2210 0.0003839614219032228\n",
      "2220 0.0003874218964483589\n",
      "2230 0.00039100131834857166\n",
      "2240 0.0003796819655690342\n",
      "2250 0.00037474752753041685\n",
      "2260 0.000370462192222476\n",
      "2270 0.00036629356327466667\n",
      "2280 0.0003663640527520329\n",
      "2290 0.00044816394802182913\n",
      "2300 0.0004311733937356621\n",
      "2310 0.0004202875425107777\n",
      "2320 0.0004146205901633948\n",
      "2330 0.0004114731273148209\n",
      "2340 0.0004098739009350538\n",
      "2350 0.00040836716652847826\n",
      "2360 0.00040775106754153967\n",
      "2370 0.0004069021379109472\n",
      "2380 0.00040690499008633196\n",
      "2390 0.0004060710489284247\n",
      "2400 0.00040591464494355023\n",
      "2410 0.00040608455310575664\n",
      "2420 0.0004071241710335016\n",
      "2430 0.0004228103207424283\n",
      "2440 0.0004118984506931156\n",
      "2450 0.00040496542351320386\n",
      "2460 0.00040153396548703313\n",
      "2470 0.0004539057845249772\n",
      "2480 0.00043867691420018673\n",
      "2490 0.00043156769243068993\n",
      "2500 0.0004293624951969832\n",
      "2510 0.00042829581070691347\n",
      "2520 0.00042904820293188095\n",
      "2530 0.000429431936936453\n",
      "2540 0.0004286247421987355\n",
      "2550 0.000429604115197435\n",
      "2560 0.00042981564183719456\n",
      "2570 0.0004300832806620747\n",
      "2580 0.0004262675647623837\n",
      "2590 0.0004274721723049879\n",
      "2600 0.00043135543819516897\n",
      "2610 0.00045185594353824854\n",
      "2620 0.00044252307270653546\n",
      "2630 0.0004417102027218789\n",
      "2640 0.00043867871863767505\n",
      "2650 0.000446568796178326\n",
      "2660 0.00043772917706519365\n",
      "2670 0.0004352324758656323\n",
      "2680 0.00043161545181646943\n",
      "2690 0.00043408278725109994\n",
      "2700 0.0004331886302679777\n",
      "2710 0.00042927172034978867\n",
      "2720 0.0004292787634767592\n",
      "2730 0.00042660898179747164\n",
      "2740 0.000429483043262735\n",
      "2750 0.00042810087325051427\n",
      "2760 0.00043341334094293416\n",
      "2770 0.0004284484020899981\n",
      "2780 0.0004298041749279946\n",
      "2790 0.00045411623432300985\n",
      "2800 0.00044249833445064723\n",
      "2810 0.00043163204099982977\n",
      "2820 0.00042409583693370223\n",
      "2830 0.0004257363034412265\n",
      "2840 0.00045586799387820065\n",
      "2850 0.0004446435777936131\n",
      "2860 0.00043841925798915327\n",
      "2870 0.00043733001803047955\n",
      "2880 0.00043589892447926104\n",
      "2890 0.00043629639549180865\n",
      "2900 0.00043326549348421395\n",
      "2910 0.000430126121500507\n",
      "2920 0.00043057434959337115\n",
      "2930 0.0004265939351171255\n",
      "2940 0.0004267263284418732\n",
      "2950 0.0004455074667930603\n",
      "2960 0.00043129283585585654\n",
      "2970 0.00042889825999736786\n",
      "2980 0.00048035199870355427\n",
      "2990 0.0004481555079109967\n",
      "3000 0.0004410111578181386\n",
      "3010 0.0004400579200591892\n",
      "3020 0.0004337382270023227\n",
      "3030 0.0004328642098698765\n",
      "3040 0.00042892753845080733\n",
      "3050 0.00043576525058597326\n",
      "3060 0.00043556236778385937\n",
      "3070 0.00042707257671281695\n",
      "3080 0.0004254115337971598\n",
      "3090 0.00044307223288342357\n",
      "3100 0.00043985803495161235\n",
      "3110 0.0004298062704037875\n",
      "3120 0.0004308744100853801\n",
      "3130 0.00042277202010154724\n",
      "3140 0.00041882856748998165\n",
      "3150 0.0004266689356882125\n",
      "3160 0.0004162746772635728\n",
      "3170 0.00041869678534567356\n",
      "3180 0.00040999113116413355\n",
      "3190 0.0004092405433766544\n",
      "3200 0.00041363469790667295\n",
      "3210 0.000402113568270579\n",
      "3220 0.0004057029145769775\n",
      "3230 0.0004088203713763505\n",
      "3240 0.00040640472434461117\n",
      "3250 0.00040818980778567493\n",
      "3260 0.0004133034381084144\n",
      "3270 0.00039985933108255267\n",
      "3280 0.0004082473460584879\n",
      "3290 0.00041871980647556484\n",
      "3300 0.00041255151154473424\n",
      "3310 0.0004351969691924751\n",
      "3320 0.00041335486457683146\n",
      "3330 0.00040104647632688284\n",
      "3340 0.0003962863120250404\n",
      "3350 0.00038295183912850916\n",
      "3360 0.00039923202712088823\n",
      "3370 0.00040193743188865483\n",
      "3380 0.00040566694224253297\n",
      "3390 0.0003909292572643608\n",
      "3400 0.0003971079713664949\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      5\u001B[0m pred_hks \u001B[38;5;241m=\u001B[39m hksNet(canvass_train_torch)\n\u001B[1;32m      6\u001B[0m loss \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmean(( (pred_hks \u001B[38;5;241m-\u001B[39m hkss_train_torch) \u001B[38;5;241m/\u001B[39m hkss_train_torch )\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m----> 7\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m opt\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m10\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/.conda/envs/TSM/lib/python3.10/site-packages/torch/_tensor.py:488\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    479\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    480\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    481\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    486\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    487\u001B[0m     )\n\u001B[0;32m--> 488\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    490\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/TSM/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(len(learning_rates)):\n",
    "    opt = torch.optim.AdamW(list(hksNet.parameters()),learning_rates[i])\n",
    "    opt.zero_grad()\n",
    "\n",
    "    pred_hks = hksNet(canvass_train_torch)\n",
    "    loss = torch.mean(( (pred_hks - hkss_train_torch) / hkss_train_torch )**2)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(i,loss.item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0026, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pred_hks_test = hksNet(canvass_test_torch)\n",
    "loss_test = torch.mean(( ( pred_hks_test - hkss_test_torch) / hkss_test_torch )**2)\n",
    "print(loss_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Randomly sample a few to see what it looks like"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.8529, 4.9785, 1.8480, 2.1630], grad_fn=<SelectBackward0>)\n",
      "tensor([3.8336, 4.9593, 1.8316, 2.1480])\n"
     ]
    }
   ],
   "source": [
    "print(pred_hks_test[15])\n",
    "print(hkss_test_torch[15])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.9131, 5.0612, 1.8715, 2.1914], grad_fn=<SelectBackward0>)\n",
      "tensor([3.8539, 4.9875, 1.8376, 2.1563])\n"
     ]
    }
   ],
   "source": [
    "print(pred_hks_test[35])\n",
    "print(hkss_test_torch[35])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.8326, 4.9901, 1.8483, 2.1524], grad_fn=<SelectBackward0>)\n",
      "tensor([3.7467, 4.8356, 1.8077, 2.1144])\n"
     ]
    }
   ],
   "source": [
    "print(pred_hks_test[60])\n",
    "print(hkss_test_torch[60])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.8649, 5.0191, 1.8528, 2.1645], grad_fn=<SelectBackward0>)\n",
      "tensor([3.7738, 4.8761, 1.8140, 2.1236])\n"
     ]
    }
   ],
   "source": [
    "print(pred_hks_test[96])\n",
    "print(hkss_test_torch[96])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
